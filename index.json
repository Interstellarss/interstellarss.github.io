[{"categories":["Notes"],"content":"Binary Translation Work on multiple instructions at once less management use sequence (of codes) without control flow change (“basic block” = BB) „dynamic BB“: ends at jump/branch instruction „static BB“ (term for compilers): ends at jump instr. or before jump target piecewise translation only when execution is required detection of instruction borders now simple (compare with static translation: what is code?) Dynamic Binary Translation Components required code cache (CC) sometimes called translation cache (TC) used to store translated basic blocks (BBs) has to be invalidated when original code is removed („unmapped“) is modified translation table maps start addresses of BBs: SPC =\u003e TPC looked up for (indirect) jumps original code and data are kept for potential accesses Code Cache properties in common with processor cashes limited size =\u003e replacement strategy (LRU, LFU, FIFO, …) processor caches have tags assoziativity cache line length coherency protocol differences to processor caches on a cache miss … differenttranslationspossibleforsameBB cachehierarchy? variable sizes of translated code interdependencies among entries (see chaining …) Code Cache: Invalidation required for self modifying code („SMC“) detection use host MMU (Memory Management Unit) guest code is write-protected on a write: invalidate all code on given page fall back: interpretation very slow with frequent modifications check for modification before every BB execution copy of original guest code required slow modification of currently running BB? Code Cache: Eviction handling Cache is full, but translation to be stored LRU evict the translated BB least recently used problems need to maintain order of uses =\u003e Overhead (time stamps / linked lists) fragmentation of cache storage – possible solution: buddy lists eviction events happen often, so need to be fast =\u003e LRU is rarely used. What is better? Goal: minimum runtime overhead Solutions: complete flush when full advantage: regular retranslation – adaptation of optimizations to current runtime behavior – old/unneeded translations get removed disadvantage – frequently used BBs have to be translated often – flush on detection of execution phase change – blocks with FIFO replacement Chaining Observation: expensive actions lookup SPC =\u003e TPC indirect jump Chaining = Linking translated BBs on known successor last guest instruction is unconditional jump conditional jump (2 cases: follow/pass-through) lookup jump targets at translation time what if successor not translated yet? on unknown successor indirect jump convert into if-then-else chains (profile targets!) return from function use shadow stack (similar to return prediction) Superblocks (SB) Motivation reduce number of jumps for given guest code (same as with chaining) larger translation units allow for more optimization possibilities (see next slides) Superblock One entry, multiple “side exits” we may have branch which go out of SB in the middle, if you have a unconditional jumps in ur original code you may just go on. I you have a conditional jump you may have a side exits at that point Combine sequence of BBs works best if the execution path of the full SB is similar to later executions (it is “hot”) =\u003e predict from past also called “trace” (“tracing JITs”) =\u003e execution path of BBs ","date":"2022-03-02","objectID":"/posts/vt/isa_emulation2/:0:1","tags":["Virtualization_Tech"],"title":"Something about ISA Emulation -- 2","uri":"/posts/vt/isa_emulation2/"},{"categories":["Notes"],"content":"Process VM Emulation of a user-level process (with possibly different ISA, different OS) – components required? – techniques? – correctness verification? Process VM with same OS, same ISA – use case? motivation process migration flexible control of used OS resources „OS-level virtualization“ enough not discussed here Virtualization of a process environment emulation of user ISA + OS environment (ABI) every guest process gets own environment guest and host OS often the same View of the host machine user processes running inside Process VM look exactly the same as host processes Wanted: easy usage of VM (automatic startup on demand) Process-level VMs provide user applications with a virtual ABI environ- ment. In their various implementations, process VMs can provide replication, emulation, and optimization. The following subsections describe each of these. Components of a Process VM Initialization – loader for code/data into guest memory – creation of tables, translation cache – redirection of exceptions ​ • for all potentially happening exceptions ​ • example: division by zero scenarios: ​ – guest does own handling ​ – guest ISA does not generate this exception Emulation Manager, translation – see ISA part emulation of OS calls – needs mapping of guest to host OS emulation of exceptions – e.g. page fault, division by zero – detection by interpreter / by hardware (OS) – reconstruction of precise state needed handling of interrupts – examples: signals, OS callbacks Types of Compatibility Strict Compatibility („Intrinsic“) – every operation of a real machine is precisely emulated by Process VM, including processor errata visible implementation behavior (e.g. „undefined“ behavior) usually not included: performance – real properties have to be known! – result: guest process is unable to detect any difference in virtual vs. real environment – complete check of VM implementation required – example: „emulation“ of Intel x86 processor by AMD HW Problems – exec has to be known completely – precise mapping can be very inefficient – high development costs Real motivation for a Process VM? – example: MS Office running on PowerPC/Linux – observation - not every operation is used - precise mapping of reality not required for execution! e.g. „don ́t care bits“ in result of a OS call – relaxation of compatibility possible Relaxed Compatibility („Extrinsic“) construction of exec\" such that exec ́ is sufficiently approximated, e.g. - only mapping for operations generated by a given compiler - implementation by similar operations (80 bit floating point emulated by 64 bit FP) „sufficiently“ defined by guest SW able to run - significantly simpler verification - ISA documentation is enough - precise mapping of operations can be replaced by more efficient, similar ones State Mapping Guest =\u003e Host Memory – direct mapping with fixed offset guest address A =\u003e host address A ́ = offset + A special case: offset = 0 – possible if guest OS allocates address ranges (i.e. user-level guest needs to be able to cope with arbitrary addresses from guest OS) – efficient emulation possible – issues available host memory smaller than guest memory 64bit guest on 32bit host =\u003e needs indirect address mapping in software (= the general resolution) Address Mapping in Software A ́ = (A \u0026 (PageLen-1)) + AddrTab[ A / PageLen] effort per access into guest memory: - temporary register + 6 host instructions + 2x host memory access - length of AddrTab: guest memory size / PageLen * pointer size host Emulation of Memory Compatibility: often a tradeoff between – performance and – compatibility Issues – how to protect VM from accesses of guest? – linear address space vs. segmentation – memory protection capabilities: Read/Write/Execute (RWE) granularity: page size – x86 can use either 4 KB or 2/4 MB – MIPS/IA-64: almost any power-of-two =\u003e Use HW if possible (e.g. mprotect + SEGF handler) Emulation of Exceptions and Interrupts Notifications from the outside that something has happen","date":"2022-02-10","objectID":"/posts/vt/process_virtual_machine/:0:1","tags":["Virtualization_Tech"],"title":"Something about Process-Virtual-Machine","uri":"/posts/vt/process_virtual_machine/"},{"categories":["Notes"],"content":"A complete ISA consists of many parts, including the register set and memory architecture, the instructions , and the trap and interrupt architecture. A virtual machine implementation is usually concerned with all aspects of ISA emulation. Here we will be focusing on (user-level) instruction emulation. Instruction set emulation can be carried out basiclly in 2 techniques: interpretation, and binary translation. Interpretation involves a cycle of fetching a source instrction, analyzing it, performing the required operation, and then fetching the next source instruction – all in software. Binary translation, on the other hand, attempts to amortize the fetch and analysis costs by translating a block of source instructions to a block of target instructions and saving the translated code for repeated use. In contrast to interpretation, binary translation has a bigger initial translation cost but a smaller execution cost. The choice of one or the otehr depends on the number of times a block of source code is expected to be executed by the guest softeware. Predictably, there are techniques that lie in between these extremes. For example, threaded interpretation eliminated the interpreter loop correstponding to the cycle mentioned earlier, and efficiency can be increased even further by predecoding the source instructions into a more efficiently interpretable intermediate form. 1 ","date":"2021-11-16","objectID":"/posts/vt/isa_emulation1/:0:0","tags":["Virtualization_Tech"],"title":"Something about ISA Emulation -- 1","uri":"/posts/vt/isa_emulation1/"},{"categories":["Notes"],"content":"Speeding up Interpretation Observation: Jumps are time consuming control conflicts in CPU pipeline lots of branch mispredictions in dispatcher What are pipeline conflicts? data/control/resource conflicts enforce pipeline stalls longer pipeline stalls longer pipeline risks longer stalls How to reduce bad effects of control conflicts? predict jump targets execute in speculative state How do branch predictors typucally work? static: on first execution (e.g. take backwards branches) dynamic: often uses table keyed by instruction address conditional: saturating counters keyed by history pattern returns: stack of recently pushed return addrresses Efficiency Guidlines for Branches “Premature optimization is the root of all evil” run performance analysis tools on finel code Reduce number of branches use inlining, also helps by specialization combine switch statments () Optimizaiton “Threaded Interpretation” Observation: Jumps are time consuming control conflicts in CPU pipeline lots of branch mispredictions in dispatcher Solution: decode next instruction at end of emulation routine of curretn instruction. Inditect vs direct This code is very similar to the indirect threaded code, except the dispatch table lookup is removed. The address of the interpreter routine is loaded from a field in the intermediate code, and a register indirect jump goes directly to the routine. Although fast, this causes the intermediate form to become dependent on the exact locations of the interpreter routines and consequently limits portability. If the interpreter code is ported to a different target machine, it must be regenerated for the target machine that executes it. However, there are programming techniques and compiler features that can mitigate this problem to some extent. Interpretation using Predecoding Motivation: Although the centralized dispatch loop has been eliminated in the indirect threaded interpreter, there remains the overhead created by the centralized dispatch table. Looking up an interpreter routine in this table still requires a memory access and a register indirect branch. It would be desirable, for even better efficiency, to eliminate the access to the centralized table. Observation: opcodes consist of multiple parts faster: one opcode (instead of op + ex_op) operands are coded in bits faster: operands aligned Properties: Space for predecoded data needed faster interpretaion significant benefit for interpretaion of CISC TPC \u0026 SPC: Why still SPC? Whenever it maybe used somewhere else You could have some kind of code which try to read the machine code itself from the PC for whatever reasons. why TPC + 1 but SPC + 4? TPC is in C, and the pre-decode array is also in C, so the compiler does the work. However, SPC + 4, if one instruction is in 4 byte. Interpretation - CISC **Potential issues with predecoding: ** much space needed better: space-tuned formats for different instructions detection of instruction borders could be data interleaved with code correct predecoding almost impossible Use a two-step process at first interpretation: do predecoding on the fly, filling predecode table all further executions: use predecoded data generated by first run [James E. Smith, Ravi Nair, Virtual Machines, 2015, ISBN:9781558609105] ↩︎ ","date":"2021-11-16","objectID":"/posts/vt/isa_emulation1/:0:1","tags":["Virtualization_Tech"],"title":"Something about ISA Emulation -- 1","uri":"/posts/vt/isa_emulation1/"},{"categories":["Notes"],"content":"Relational Model Definition relational database: a set of relations Relations : Schema: specifics name of relation, plus name and type of each column Student(sid:string, name:string, login: string, gpa: real) Instance:a table, with rows and columns Number of rows: cardinality Number of fields: degree or arity can think of as set of tuples or records Domain constrains the values that appear in a column must be drawn from the domain associated with that column relation schema: $R(f_1:D_1,…,f_n:D_n)$ $Dom_i$:set of values with domain names $D_i$ An instance of R that specifies the domain constraints in the shcema is a set of tuples with n fields: $$ Verfeinerung des relationalen Schemas “Das im Initialentwurf erzeugte relationale Schema lässt sich oftmals noch verfeinern. Dabei werden einige der Relationen eliminiert , die für die Modellierung von Beziehungstypen eingeführt worden waren. Dies ist aber nur für solche Relationen möglich, die die allgemeinen 1:1-, 1:N- oder N:1-Beziehungen repräsentieren. Die Elimination der Relationen, die die allgemeinen N:M-Beziehungstypen repräsentieren, ist nicht sinnvoll und würde i.A. zu schwerwiegenden “Anomalien“ führen. Bei der Eliminierung von Relationen gilt es folgende Regel zu beachten: Nur Relationen mit gleichen Schlüssel zusammenfassen!” 1 Vermeidung von Null-Werten ","date":"2020-11-15","objectID":"/posts/database/datenbank-3/:0:1","tags":["Database"],"title":"Relational Model and Relational Algebra","uri":"/posts/database/datenbank-3/"},{"categories":["Notes"],"content":"Relational Algebra $\\cup$ Union - Difference R和S的差是由属于R但不属于S的元组组成的集合。记作： $$ R - S = \\lbrace t | t \\in R \\and t \\notin S \\rbrace $$ $\\delta$ Selection 选择运算是根据一定条件F在给定的关系R中选取若干个元组，组成一个新关系，记作： $$ \\delta_F(R) = \\lbrace t | t \\in R \\and F(t)= true \\rbrace $$ 其中，$\\delta$为选择运算符，F表示选择事件，它是由运算对象（属性名、常数、简单函数）、算术比较运算符（\u003c,≤,\u003e,≥,=,≠）和逻辑运算符（$\\land,\\lor,\\neg $）连接起来的逻辑表达式，取值为“true”或“false”， 其基本形式为 $$ $$ $\\pi$ Projection 投影运算也是单目运算，是从一个关系中选取某些属性（列），并对这些属性重新排列，最后从得出的结果中删除重复的行，从而得到一个新的关系。即对关系从列的角度进行的垂直分解运算，从左到右按照指定的若干属性及顺序取出相应列，删去重复元组。 $\\Join$ Join (Verbund) 连接运算是二目运算，是从两个关系的笛卡儿积中选取属性间满足一定条件的元组，组成新的关系，连接又称θ连接。 Der natürliche Verbund(自然连接) $\\div$ Division 除运算是二目运算，给定关系 R（X，Y）和 S（Y，Z），其中 X，Y，Z 为属性或属性集。R中的Y和S中的Y可以有不同的属性名，但必须出自相同的域集。 [Kemper, Alfons and Eickler, André, Datenbanksysteme: Eine Einführung, 7.,ISBN:9783486590180] ↩︎ ","date":"2020-11-15","objectID":"/posts/database/datenbank-3/:0:2","tags":["Database"],"title":"Relational Model and Relational Algebra","uri":"/posts/database/datenbank-3/"},{"categories":["Notes"],"content":"We know that there are different ways to represent “rotation”, and the different ways have their limitations. ","date":"2020-11-10","objectID":"/posts/imge/imge-1_en/:0:0","tags":["Intergration Methods and Devices"],"title":"Different Approaches to 「Rotation」","uri":"/posts/imge/imge-1_en/"},{"categories":["Notes"],"content":"Quaternions Quaternions are usually notated as H (for Hamiltonian reasons). From wikipedia, “Explicitly, quaternions are non-commutable extensions of complex numbers. If the set of quaternions is considered as a multidimensional real space, the quaternions represent a four-dimensional space, as opposed to a two-dimensional space of complex numbers. \" We can express it as a four-dimensional vector. $$ q = \u003cw,x,y,z\u003e q = w + xi + yi +zk q = s + v $$ – w,x,y,z are real numbers，v is a 3-dimentional vector(e.g. a 3D point), – i,j,k complex numbers (“quaternion units”) with $ i^2 = j^2=k^2=ijk=-1 $ Perhaps looking at the above equation may still be confused as to what each parameter represents, and seeing these parameters there is no specific imagination in the mind about the rotation. This is actually quite normal, if you can visualize it in your mind instead of some abnormal :) In any case, quaternions actually belong to the “remnants” of “spatial magic”. Why is that? The following is a deeper study of the quaternion by looking at it, or more accurately, the projection of the quaternion hypersphere in three. The reason why quaternions are difficult to understand is that it is a four-dimensional representation. But it is not impossible, because we usually use the unit quaternion to represent the rotation, so we only need to focus on the unit hypersphere in our thinking, and then we can more easily obtain its spherical polar plane projection (stereographic projection) in three dimensions. Projection of the unit circle in one dimension We know that the projection of the unit circle in one dimension as a two-dimensional space is an infinitely extended line. Suppose we have a plane coordinate system, with the x-axis as the real axis and the y-axis as the imaginary axis, and the intersection of the line from the point - i and any point on the circle with the x-axis constitutes its projection. It is worth noting that the line coinciding with the x-axis is only one of the projections of the unit circle in this two-dimensional space, and there is no way to represent the projections of other points in the two-dimensional space in this one-dimensional space.。 Projection of the unit ball in two dimensions Similarly, the projection of the unit sphere in two dimensions can be viewed similarly. Suppose we have the i-axis, the j-axis, two imaginary axes forming a plane, and the real axis coinciding with the z-axis. Then in a similar technique to the previous projection, we can describe the rotation of three in terms of the spherical polar plane projection. For each point on the unit sphere, we can connect it to -1, and the intersection of this line with the ij plane is the projection point in two dimensions, so that 1 on the real number axis will be projected at the origin of the plane, the point in the northern hemisphere will be projected inside the unit circle in the ij plane, and the point in the southern hemisphere will be projected in the unit circle outside the unit circle, and at infinity in any direction will be the projection of -1. The rotation is the process of “lines” gradually enclosing “circles” and “circles” gradually expanding into “lines”. Projection of unit hypersphere in 3D space Next is the focus of our attention. With the above layouts, it is easy to understand why quaternions are composed of a real term with three imaginary terms. For a spherical polar plane projection of the unit sphere in four dimensions, the 1 of the real number axis projects the origin of the ijk coordinate system. When the four-dimensional hypersphere is projected into three-dimensional space, it intersects the three-dimensional unit sphere in the same position as the three-dimensional space, and this sphere corresponds to the pure quaternion, i.e., the real part is zero. The real part between 0 and 1 is projected inside this three-dimensional sphere, while the real part less than 0 is projected outside the three-dimensional sphere, and -1 is projected ","date":"2020-11-10","objectID":"/posts/imge/imge-1_en/:0:1","tags":["Intergration Methods and Devices"],"title":"Different Approaches to 「Rotation」","uri":"/posts/imge/imge-1_en/"},{"categories":["Notes"],"content":"Explore the story of the network layer in the OSI model of computer networks.","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"Here we will recognize the two different services provided by the network layer and the core of the network layer, namely the IP protocol. We can understand: the concept of virtual interconnection networks. the relationship between IP addresses and physical addresses the traditional classification of IP addresses (including subnet masks) ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:0","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"Address Resolution Protocol ARP In practical applications, we often encounter the problem that we already know the IP address of a machine (host or router) and need to find out its corresponding hardware address. The address resolution protocol ARP is used to solve such a problem. Since it is the IP protocol that uses the ARP protocol, it is usually classified as a network layer. However, the purpose of the ARP protocol is to resolve the hardware address used at the data link layer from the IP address used at the network layer. The following is an overview of the main points of the ARP protocol. We know that the network layer uses IP addresses, but when transmitting data frames over the links of the actual network, the hardware address of that network must eventually be used. However, there is no simple mapping between IP addresses and the hardware addresses of the network below due to the different formats (for example, IP addresses have 32 bits, while the hardware addresses of a LAN are 48 bits). In addition, new hosts may often be added to a network, or some hosts may be removed. Changing network adapters can also cause the hardware addresses of hosts to change. Address Resolution Protocol ARP solves this problem by storing a mapping table from IP addresses to hardware addresses in the host’s ARP cache, and this mapping table is also dynamically updated (added or deleted on timeout) frequently. Each host has an ARP cache, which contains the mapping table from IP addresses to hardware addresses of the hosts and routers on the LAN, and these are the addresses that the host knows at present. How does the host know these addresses? We can use the following example to illustrate. When host A wants to send an IP datagram to a host B on the LAN, it first checks if there is an IP address of host B in its ARP cache. If there is, it will find out the corresponding hardware address in the ARP cache, and then write the hardware address to the MAC frame, and then send the MAC frame to this hardware address through the LAN. There may also be items where the IP address of host B cannot be checked. This may be because Host B has only just entered the network, or it may be that Host A has just been powered up and its cache is still empty. In this case, Host A runs ARP automatically and then follows the steps below to find out the hardware address of Host B. The ARP process broadcasts and sends an ARP request packet on the local area network. An example of the main content of the ARP request packet is: “My IP address is 209.0.0.5 and my hardware address is 00-00-C0-15-AD-18. I want to know the hardware address of the host with IP address 209.0.0.6.” This ARP request packet is received by all ARP processes running on all hosts on this LAN. The IP address of host B matches the IP address to be queried in the ARP request packet, so it receives this ARP request packet and sends an ARP response packet to host A, and writes its own hardware address in this ARP response packet. Since the IP addresses of all the other hosts do not match the IP address in the ARP request packet, they ignore the ARP request packet. The main content of the ARP response packet is: “My IP address is 209.0.0.6, and my hardware address is 08-00-2B-00-EE-0A.” Note that although the ARP request packet is sent broadcast, the ARP response packet is plain unicast, i.e., it is sent from a source address to a destination address. Once host A receives the ARP response packet from host B, it writes the mapping from host B’s IP address to the hardware address in its ARP cache. When host A sends a datagram to B, it is likely that host B will have to send another datagram to A shortly thereafter, and thus host B may also have to send an ARP request packet to A. To reduce the amount of traffic on the network, host A writes its IP address to the hardware address mapping into the ARP request packet when it sends its ARP request packet. When host B receives A’s ARP request packet, it writ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:1","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"IPv4 The format of an IP datagram can illustrate what functions the IP protocol has. An IP datagram consists of two parts: the header and the data. The first part of the header is a fixed length of 20 bytes, which is mandatory for all IP datagrams. Following the fixed part of the header are optional fields that are variable in length. The following describes the meaning of each field in the header. Version Occupies 4 bits and refers to the version of the IP protocol. The version of the IP protocol used by both communicating parties must be the same. The current widely used IP protocol version number is 4 (i.e. IPv4). Header Length The maximum decimal value that can be represented is 15. Please note that the unit of the number represented by the initial length field is a 32-bit word (one 32-bit word is 4 bytes long). Since the fixed length of the IP prefix is 20 bytes, the minimum value of the prefix length field is 5 (i.e., the binary representation of the prefix length is 0101). And when the prefix length is the maximum value 1111 (i.e., 15 in decimal), it indicates that the prefix length reaches a maximum of 15 32-bit words long, i.e., 60 bytes. When the IP packet’s initial length is not an integer multiple of 4 bytes, it must be padded using the final padding field. Therefore, the data portion of an IP datagram always starts at an integer multiple of 4 bytes, which is more convenient when implementing IP protocols. The disadvantage of limiting the initial length to 60 bytes is that sometimes it may not be enough. However, this is done in the hope that the user will minimize the overhead. The most common prefix length is 20 bytes (i.e., a prefix length of 0101), at which point no options are used. Differentiated Services It takes up 8 bits and is used to get better service. This field was called Service Type in the old standard , but it was never actually used. Total Length The total length is the length of the sum of the header and data in bytes. The total length field is 16 bits, so the maximum length of a datagram is 2 16 - 1 = 65535 bytes. However, in practice, it is very rare to transmit such a long datagram. ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:2","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"Flow of IP layer forwarding packets We know that there is a routing table in the router, but if the routing table points to each host how to forward, that would make the routing table too large, but if the routing table only points to a network how to forward, then the routing table will only include the number of network items, which greatly reduces the need to store items, we do not have to care about the specific topology of a network and the specific number of devices connected to the network, because from one router to the next router forwarding. The most important items in the routing table are the following two pieces of information: $$ (Destination network address, next-hop address) $$ ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:3","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"ICMP(Internet Control Message Protocol) In order to forward IP datagrams more efficiently and to improve the chances of successful delivery, the ICMP protocol is used at the network layer. The ICMP protocol allows hosts or routers to report errors and provide reports on exceptions. It is important to note that ICMP is not a high-level protocol, but rather remains a network layer protocol, as ICMP messages are packed into IP datagrams as part of the data. “TOS” and “Protocol” are related to ICMP in the introduction to the packet header structure. The description given in the professor’s slide is also great: Das Internet Control Message Protocol (ICMP) dient dazu, • in derartigen Fällen den Absender über das Problem zu benachrichtigen und • stellt zusätzlich Möglichkeiten bereit, um z. B. • die Erreichbarkeit von Hosts zu prüfen („Ping“) oder • Pakete umzuleiten (Redirect). Types of ICMP messages ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:4","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"IPv6 IPv6 Basic Header After IPv4 ran out of addresses, we naturally needed to expand the IP address block, hence IPv6. IPv6 still supports connectionless transport, but refers to the protocol data unit PDUs as packets, rather than IPv4 datagrams. The main changes in IPv6 are as follows: Larger address space, increasing the 32-bit IPv4 address space to four times that of IPv4, i.e., to 128 bits. Extended address hierarchy. Flexible prefix format. Improved options. 5. Allows protocols to continue to expand. 6. IPv6 prefixes are now 8-byte aligned (i.e., the prefix length must be an integer multiple of 8 bytes), whereas IPv4 prefixes were 4-byte aligned. Compared to IPv4, IPv6 has changed some fields in the header as follows: The length field of the header has been removed, since the length of the header is fixed (40 bytes) The Type of Service (TOS) field has been removed, since the Priority and Flow Label Number fields implement the function of the Type of Service field The total field length has been removed in favor of the payload length field Eliminated the Identification, Flag and Slice Offset fields, but the role is the same The protocol field has been removed Check sum field has been removed Removed optional fields IPv4-Header (oben) und IPv6-Header (unten) im Vergleich IPv6 Basic Prefix Segments Role IPv6 Header version 4 bits, specifies the protocol version, for IPv6 this field is 6 traffic class 8 bits. This is to distinguish the class or priority of different IPv6 datagrams. We are currently experimenting with different traffic performance. flow label 20 bits. A new mechanism in IPv6 is to support resource preallocation and allow routers to associate each datagram with a given resource allocation. IPv6 introduces the abstract concept of flow. A “flow” is a series of datagrams () on an interconnected network from a specific source to a specific destination (unicast or multicast), and the routers in the path of this “flow” are guaranteed the specified quality of service. All datagrams belonging to the same stream have the same stream label. Therefore, stream tagging is particularly useful for real-time audio/video data delivery. 4. payload length is 16 bits. It specifies the number of bytes in the IPv6 datagram in addition to the base header (all extended headers are counted as part of the payload). The maximum value of this field is 64KB (65535 bytes) next header 8 bits. It is equivalent to the optional field of the IPv4 protocol field. hop limit 8 bits. Used to prevent datagrams. Source address 128 bits. It is the address of the sender side of the datagram. 8. Destination address 128 bits. It is the IP address of the receiving end of the datagram. Let’s introduce the extended initials of IPv6. As you know, if an IPv4 datagram uses options in its header, each router along the path of the datagram must check each of these options, which slows down the processing of the datagram by the router. IPv6 puts the functionality of the options in the original IPv4 header in the extended header, and leaves the extended header to the hosts at the source and destination at each end of the path, while none of the routers** through which the datagram passes process the extended header** (with the exception of one header). (with the exception of one, the hop-by-hop option for extended headers), which ** greatly improves the processing efficiency of the router **. We will probably encounter these six extended headers: (1) hop-by-hop option; (2) routing; (3) fragmentation; (4) identification; (5) encapsulation security payload; and (6) destination option. Each extension header consists of a number of fields, and they vary in length. However, the first field of all extension headers is the 8-bit “next header” field. The value of this field indicates what field follows the extension header. When multiple extension headers are used, they should appear in the above order. High-level headers are always placed last. IPv6 Addresses n general, ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:5","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"Routing Longest Prefix Matching When using CIDR, since this notation of network prefix is used, the IP address consists of two parts: network prefix and host number, so the items in the routing table have to be changed accordingly. At this time, each item consists of “network prefix” and “next-hop address”. But when looking up the routing table you may get more than one match . This raises the question: which route should we choose from these matches? The correct answer is: the route with the longest network prefix should be selected from the match results. This is called longest-prefix matching, because the longer the network prefix, the smaller the address block, and therefore the more specific the route. Longest-prefix matching is also known as longest matching or best matching. Die Routingtabelle wird von längeren Präfixen (spezifischeren Routen) hin zu kürzeren Präfixen (weniger spezifische Routen) durchsucht. Der erste passende Eintrag liefert das Gateway (Next-Hop) eines Pakets. Diesen Prozess bezeichnet man als Longest Prefix Matching. ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:6","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"first draft","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"这次带来的是提升篇，其实就是seminar要交的paper的first draft，倒也算是偷懒的一种方式吧。 ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:0:0","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"A brief Overview on Shared Memory Parallelism in Parallel Computing ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:0","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"1. Why parallel? To speed up, while we are facing the limitation of current transistors and increasing energy consumption. Now that we know it is necessary and lots of privilege besides you need to reconstruct your program yourself rather than automatically distributed by APIs in a serial way. So, it then leads to the following question: how do we write parallel programs? Truth be told, there are number of possible answers to this question, while most of them share the idea of partitioning the work among cores. The two commonly used approach for this: task-parallelism and data-parallelism. In task-parallelism, we partition the problems into separately tasks that will be carried out in cores. While in data-parallelism each core carries out roughly similar operations on its part of data. ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:1","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"2. As it was mentioned above, when we write programs that are explicitly parallel, we will be focusing on two major types of parallel systems: shared-memory and distributed-memory. What is the idea of shared memory system? In a shared memory system, processors are connected via an interconnection network, so that every core can access ach memory location. And there are different hardware structures in implementing this idea. In shared-memory system all processors are either connected directly to the main memory or have their own memory blocks and are accessible to each other through special hardware build into the processors. Anyway, shared memory parallel computers may have different implementations, but generally have in common the ability for all processors to access all memory as global address space, which means multiple processors can operate independently but share the same memory resources. Furthermore, changes in a memory location operated by one processor are visible to all other processors. Interconnection topologies Thus, the first type of system is called a uniform memory access, or UMA, system, while the second type is called a nonuniform memory access, or NUMA, system. In UMA systems the time for every core to access the memory locations is the same, while in NUMA access time from one cache to distributed data parts varies as topology place. The NUMA Systems In a Nonuniform Memory Access architecture, each CPU has a memory module physically next to it, Advantages: Global address space provides a user-friendly programming perspective to memory Data sharing between tasks is both fast and uniform due to the proximity of memory to CPUs Disadvantages: Primary disadvantage is the lack of scalability between memory and CPUs. Programmer responsibility for synchronization constructs that ensure “correct” access of global memory. Interconnection networks This is important in implementing hardware of shared-memory parallelism: the efficiency of data exchange between memory and processors have huge impact on final execution time. Shared-memory interconnects Here we will explain two most widely used interconnects in shared-memory systems: buses and crossbars. The key property for a bus is that the devices connected to it share the communication wires. Since the amount of our cores is not many, it is more flexible to use a bus. However, the expected performance decreases as the number of cores connected to the bus increases. Thus, buses are replaced by switched interconnects in a more complicated shared-memory system. As name suggests, switched interconnects use switches to control the data among the connected devices. (Figure of crossbars) The individual switches can be shown as one from following figure (Figure of crossbar status) Crossbar switches are too expensive for large -scale systems, but are useful in some small systems. However, we can simplify the idea, that leads us to Omega (or Delta) Interconnects, which is similar to crossbars, but with fewer paths. (Figure of Omega Interconnects) Cache Issues Before we go further on this topic, let us firstly recall the idea of caching. To solve the problem of redundant time of processors accessing data in main memory we added block of Cache Coherency Explanation: To understand these issues, suppose we have a shared-memory system with two cores, each of which has its own private data cache. As long as the two cores only read share data, there is no problem. For example, suppose that x is a shared variable that has been initialized to value 3. In one cache we have the instruction that changes the value of x to 7, while in another cache at the same time also have operation concerning the value of x, which leads to the problem of 3 or 7? (could be a figure involving things describes above) However, this will be unpredictable situation when situations mentioned above occur regardless of whether the system using which policy among processors, because this occurs in caches with in ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:2","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"3. OpenMP is an API designed for programming shared-memory parallel programming. The MP in OpenMP stands for “multiprocessing”. Something good about OpenMP is that the programmer does not need to specify how each thread behave explicitly, which suggests that OpenMP allows the programmer to simply mark that the block of code should be executed in parallel, and the exact determination of which thread should execute them is handed to the compiler and the run-time system. In other word, OpenMP requires more compiler support rather works like a library of functions. This convenience in writing parallel program does not come at no cost: we give away the power to program virtually any possible thread behaviour in exchange. (give an example of a program in OpenMP, then introduce the basic rules of writing program with OpenMP) ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:3","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"4.Usage in high performance computing Parallelism not always make the execution faster, sometimes the more parallelism we had, the slower the program ran, which means we need to reconsider the task before we choose to implement it parallelly. ​ (table with run time of Dijkstra with 1000 nodes but different number of threads) ​ (table with run time of Dijkstra with 25000 nodes but different number of threads) Possible way to improve our performance when writing parallel program with OpenMP on shared-memory systems: False sharing could be a problem. Example to this: analysis the execution of a Matrix multiplies a vector. $Y = A * x$ #pragma omp parallel for num_threads(thread_count) \\ default(none) private (i, j) shared(A, x, y, m, n) for(i =0; i\u003c m; i++){ y[i] = 0.0; for(j = 0; j \u003c n; j++){ y[i] += A[i][j] * x[j]; } } We will compare the performance of the matrix 8000000 x8 or 8000000 x 8 or 8 x 8000000 (efficiency table here) Although we may face much larger numbers in high performance computing Why is multi threads worse than less threads? Suppose for the moment that threads 0 and 1 are assigned to one of the processors and threads 2 and 3 are assigned to the other. Also suppose that for the 8x8,000,000 problem all of y is stored in a single cache line. Then every write to some element of y will invalidate the line in the other processor’s cache. For example, each time thread 0 updates y[0] in the statement. $y[i] += A[i][j] * x[j];$ if thread 2 or 3 is executing this code, it will have to reload y. Each thread will update each of its components 8,000,000 times. We see that with this assignment of threads to processors and components of y to cache lines, all the threads will have to reload y many times. This is going to happen in spite of the fact that only one thread accesses any one component of y, for example, only thread0 accesses $y[0]$. (Explain why false sharing may not be a problem here) Possible ways of avoiding false sharing in the matrix multiplication program: One possible solution is to “pad” y vector with dummy elements in order to guarantee that any update by one thread won’t influence another cache line. Another alternative way is to have its own private storage during the loop, and then update the shared storage when iterations are done. ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:4","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"a small algorithms for practical course for real-time computer graphics","date":"2020-05-21","objectID":"/posts/computer_graphics/implementation-about-diamond-square/","tags":["real-time computer graphics"],"title":"Generation of height maps based on diamond-square-algorithm in C++","uri":"/posts/computer_graphics/implementation-about-diamond-square/"},{"categories":["Notes"],"content":"其实很多时候我们看到的这些渲染真的是很神奇的事，比如一些terrain看似是三维的，其实可以由二维产生，即为每一个像素点赋予一个高度值，这些高度值构成的点列又可以转换为一张二维图像来存储，需要的时候，相应的地形可以由此图片生成。淡淡的想，一些所谓的空间法术，大概也就是这样了吧。高度图作为维度空间法术的入门想来也是极好的。 至于法术的入门，我们只关注用正态分布或diamond square来赋值，terrain generator中关于color和normal的值图像我们后面会聊到。 下面来看代码： #include \u003ciostream\u003e#include \u003ctchar.h\u003e#include \u003cwinnt.rh\u003e #include \u003csstream\u003e#include \u003crandom\u003e #include \u003cSimpleImage.h\u003e#include \u003cTextureGenerator.h\u003e#include \u003ctime.h\u003e #include \u003cvector\u003e //#pragma comment(lib, \"GEDUtilsd.lib\") //Declare functions float squareStep(float* field, int x, int y, int reach, int width); float diamondStep(float* field, int x, int y, int reach, int width); float random(float min, float max); //GEDUtils::SimpleImage transfer(float* field, int width); void smoothArray(float* field, int64_t width, int64_t height); void diamondSquare(float* field, int size, int width, float roughness); using namespace std; // Define a macro for easier access to a flattened 2D array #define IDX(x, y, w) ((x) + (y) * (w)) //Define the size of an array #define ARR_LEN(array, length){length = sizeof(array)/sizeof(array[0]);} /* int main() { std::cout \u003c\u003c \"Hello World!\\n\"; } */ //use for debugging void printArray(float* field, uint64_t width, uint64_t height) { for (uint64_t y = 0; y \u003c height; y++) { for (uint64_t x = 0; x \u003c width; x++) std::cout \u003c\u003c field[IDX(x, y, width)] \u003c\u003c \" \"; std::cout \u003c\u003c std::endl; } } int _tmain(int argc, _TCHAR* argv[]) { //int len; /* ARR_LEN(argv, len); if (argc != len) { throw \"invalid length\"; } */ cout \u003c\u003c endl \u003c\u003c \"argc = \" \u003c\u003c argc \u003c\u003c endl; cout \u003c\u003c \"Command line arguments received are:\" \u003c\u003c endl; for (int i = 0; i \u003c argc; i++) cout \u003c\u003c \"argument \" \u003c\u003c (i + 1) \u003c\u003c \": \" \u003c\u003c argv[i] \u003c\u003c endl; if (_tcscmp(argv[1], TEXT(\"-r\")) != 0 || _tcscmp(argv[3], TEXT(\"-o_height\")) != 0 || _tcscmp(argv[5], TEXT(\"-o_color\")) != 0 || _tcscmp(argv[7], TEXT(\"-o_normal\")) != 0) { throw \"do not match '-r' or '-o's\"; } //alternative way for resolution //std::wstringstream wsstream; //wsstream \u003c\u003c argv[2]; //int resolution; //wsstream \u003e\u003e resolution; int resolution = _tstoi(argv[2]); if (resolution \u003c= 0) { throw \"\u003c= 0 for resolution\"; } //preparation for normal mapping std::default_random_engine e; std::normal_distribution\u003cfloat\u003e n(0, 1); int length = resolution * resolution; float* field = new float[length]; //mapping normal distribution for (int y = 0; y \u003c resolution; y++) { for (int x = 0; x \u003c resolution; x++) { float value = n(e); while (value \u003c 0.0f || value \u003e 1.0f) { value = n(e); } field[IDX(x, y, resolution)] = value \u003e= 0 ? value : -value; } } //name of height file wstring wsh(argv[4]); const wchar_t* cstrh = wsh.c_str(); //name of color file wstring wsc(argv[6]); const wchar_t* cstrc = wsc.c_str(); //name of normal file wstring wsn(argv[8]); const wchar_t* cstrn = wsn.c_str(); //generate a texture generator GEDUtils::TextureGenerator texGen(L\"..\\\\..\\\\..\\\\..\\\\external\\\\textures\\\\gras15.jpg\", L\"..\\\\..\\\\..\\\\..\\\\external\\\\textures\\\\ground02.jpg\", L\"..\\\\..\\\\..\\\\..\\\\external/textures/kork02.jpg\", L\"..\\\\..\\\\..\\\\..\\\\external\\\\textures\\\\rock1.jpg\"); try { //this is for normal distribution /* //copy the created normal distribution values to the heightfield GEDUtils::SimpleImage heightImage = GEDUtils::SimpleImage::SimpleImage((UINT)resolution, (UINT)resolution); for (int y = 0; y \u003c resolution; y++) { for (int x = 0; x \u003c resolution; x++) { heightImage.setPixel(x, y, field[IDX(x, y, resolution)]); } } //save the generated heightField if (!heightImage.save(cstrh)) { throw \"Could not save heightField image\"; } // Load height image into the image test //GEDUtils::SimpleImage test(cstrh); //create a vector to store vector\u003cfloat\u003e vectorHeight(field, field + length); //this is for normal distribution //texGen.generateAndStoreImages(vectorHeight,resolution - 1,cstrc,cstrn); delete[] field; */ //this is for diamond square int length2 = (resolution + 1) * (resolution + 1); float* field2 = new float[length2]; //assign random value to the corners field2[IDX(0, 0, resolution + 1)] = random(0, 1); field2[I","date":"2020-05-21","objectID":"/posts/computer_graphics/implementation-about-diamond-square/:0:0","tags":["real-time computer graphics"],"title":"Generation of height maps based on diamond-square-algorithm in C++","uri":"/posts/computer_graphics/implementation-about-diamond-square/"},{"categories":["Notes"],"content":"有穷自动机笔记2","date":"2020-05-14","objectID":"/posts/theo/something-about-dfa-and-nfa-2/","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(二)","uri":"/posts/theo/something-about-dfa-and-nfa-2/"},{"categories":["Notes"],"content":"我们从(一)可以认识到DFA和NFA是什么样子的，并且知道它们之间的等价性，在“从有穷自动机到正则语言”一篇中我们知道DFA，NFA和正则表达式及正则语言中的等价性，同时还有一些工具如泵引理和Ardens Lemma来对它们或证明或转换。目前我们在‘术’的层面有一些进展，不妨也在或‘道’或‘术’的层面深入一些。 不妨去想一想为什么会要有穷自动机和正则语言？它们又可以用来干些什么呢？ 用Tobias教授的话来说，是一个 “Entscheidungsverfahren”, 也就是一种“判断过程”，对于已有的这些有穷自动机或是正则表达式，如何判断它们是否具备某一个属性X呢？这么说或许还是太抽象了，我们用具体的问题来说：如果有一个 D ，D是一个DFA 或是 NFA 或是 RE 或是 rechtlineare Grammatik …, 要判断如下问题： Wortproblem: 判断一个词是否被D所接受识别；Leerheitproblem： D接受的语言是否是空集；Endlichkeitsproblem： D所产生的语言是有限的还是无限的；Äquivalenzproblem: 对于 D1 和 D2 它们是否等价。那么如何判断这些问题是可判断的呢，这也就涉及到理论计算机比较核心的内容了：如果有一个算法可以在有限时间内给出正确的输出，那么就是可判断的。 现在不妨看看上述四种问题的具体判断： Lemma 3.36 Das Wortproblem ist für ein Wort w und DFA M in Zeit O(|w| + |M|) entscheidbar. Lemma 3.37 Das Wortproblem ist für ein Wort w und NFA N in Zeit O($|Q|^2$|w| + |N|) entscheidbar. Beweis： Sei Q = {1, … ,s}, $q_0 = 1 und w = a_1 … a_n$. S:= {1} for i := 1 to n do S:= $\\cup _{j \\in S} \\delta (j, a_i)$ return ($S \\cap F \\neq \\emptyset$) Lemma 3.38 Das Leerheitsproblem ist für DFAs und NFAs entscheidbar (in Zeit O(|Q||$\\Sigma$|) bzw. O($|Q|^2 |\\Sigma|$)). Beweis: L(M) = $\\emptyset$ gdw kein Endzustand von $q_0$ erreichbar ist. Dies ist eine einfache Suche in einem Graphen, die jede Kante maximal ein Mal benutzen muss. Ein NFA hat $\\leq |Q|^2|\\Sigma|$ Kanten, ein DFA hat $\\leq |Q||\\Sigma|$ Kanten. Ist $\\Sigma$ fix, z.B. ASCII, so wird daraus O($|Q|^2$) bzw O(|Q|). Lemma 3.39 ","date":"2020-05-14","objectID":"/posts/theo/something-about-dfa-and-nfa-2/:0:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(二)","uri":"/posts/theo/something-about-dfa-and-nfa-2/"},{"categories":["Notes"],"content":"数据链路层介绍","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer/","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/computernetwork/data-link-layer/"},{"categories":["Notes"],"content":"连接的特征（Verbindungscharakterisierung） 两个节点之间的连接会有如下的各种属性：传输速率（Übertragungsrate），传输延迟（Übertragungsverzögerung），传输方向（Übertragungsrichtung），以及多路访问或多路复用（Mehrfachzugriff (Multiplexing)）。 ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer/:1:0","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/computernetwork/data-link-layer/"},{"categories":["Notes"],"content":"点对点的信道 Übertragungsrate 数据在信道上面被放置的时间我们称作Serialisierungszeit，记作 $t_s$, L 为需要传输的数据大小，r 为传输速率，于是 $$ t_s = \\frac{L}{r} $$ 这里的$t_s$可以认为是传输时延，传输这些数据会要用到的时间。 Ausbreitungsverzögerung 顾名思义，传播延迟，与上面的不一样的是这是信号在信道中从一段到另一端会花的时间： $$ t_p = \\frac{d}{vc_0} $$ $c_0$即为光速，v为一个常数， 与传输介质有关。 Gesamtverzögerung $t_d 即是总的延迟为上面两项的和$。 Bandbreitenverzögerungsprodukt 这个就是所谓的宽带延迟，正是因为在信道中的传播不可避免的需要时间，我们需要一定容量来存储： $$ C = t_p r = \\frac{d}{vc_0} r $$ 单位是bit。 Übertragungsrichtung Übertragungsrichtung ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer/:1:1","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/computernetwork/data-link-layer/"},{"categories":["Notes"],"content":"广播信道的数据链路层 一般来说这里我们会用到时分复用（Zeitmultiplex），即用一个信道实现不同端口之间的通信，基于分组网络（以太网，无线局域网）中的非确定性方法（并发访问）。 至于复用我想教授的slide上面的图已经很清楚了： Multiplexing ALOHA and Sorted ALOHA CSMA, CSMA/CD, CSMA/CA CSMA 指的其实是 Carrier Sense Multiple Access, 是对于前面sorted ALOHA的一种简单的优化，也即“listen before talk”。 CSMA/CD 中文翻译也即 载波监听碰撞监听多点接入/碰撞检测，以太网即用此协议，分为载波监听和碰撞检测两部分：即“发送前先监听”，每个站在发送数据前要先检测一下总线上是否有其他站在发送数据，如果有，暂时不发送数据，等待信道空闲时再发送，总线上没有“载波”，这里只是一个习惯称呼；-碰撞检测：即“边发送边监听”，适配器边发送数据边检测信道上的信号电压，以便判断自己在发送数据时其他站是否也在发送数据。同时发送数据时，总线上的信号电压变化幅度大，超过一定门限值时，就认为总线上至少两个站同时在发送数据，表明有碰撞。这时总线上的信号失真，无法恢复。所以，每一个正在发送数据的站，一旦发现有碰撞，适配器就要立即停止发送，以免浪费网络资源，等待一段随机时间后再发送。 如果考虑上信号在链路上的传播时延，那么过程类似这样； CSMA/CD 每个端点在自己发送数据之后的一小短时间内，存在着遭遇碰撞的可能性，这段时间最长为两个单程最长时间，将这个时间成为“争用期”，只有通过争用期的“考验”，才能肯定这次发送不会发生碰撞。正是因为这个原因，以太网规定了数据帧的最小长度即64字节，所有小于此长度都认为是碰撞导致的丢弃帧。当我们接收到至少64字节我们就可认为这之间没有碰撞。 CSMA/CD不能同时进行发送和接受，因此是Halbdulplex，也就是半双工协议，即双向交替通信。 CSMA/CA 在有线连接的局域网中就不能使用CSMA/CD（无线局域网中），因为即使发送的信息足够长, 也不能总是检测到碰撞。这里的CA其实就是colision avoidance，发送包的同时不能检测到信道上有无冲突，只能尽量“避免”。例如，如果计算机A和计算机C同时给计算机B发送一个控制消息，它们将同时到达计算机B，导致冲突的发生。当这种冲突发生时，发送者可以随机等待一段时间，然后重发控制消息。因为控制消息比数据帧要短得多，所以发生第二次冲突的可能性也要比传统以太网要小很多。最终将有一个控制消息正确到达，然后计算机B发送一个响应消息。通常CSMA/CA利用ACK信号来避免冲突的发生，也就是说，只有当客户端收到网络上返回的ACK信号后才确认送出的数据已经正确到达目的。 教授slide中是这样说的： Wenn Medium frei, übertrage mit Wahrscheinlichkeit p oder verzögere mit Wahrscheinlichkeit 1 − p um eine feste Zeit dann 1. Wenn Medium belegt, warte bis frei, dann 1. ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer/:1:2","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/computernetwork/data-link-layer/"},{"categories":["Notes"],"content":"封装成帧 封装简单说就是事先对数据包进行拆分和打包，在所发送的数据包上附加上目标地址，本地地址，以及一些用于纠错的字节等。对数据包进行处理时通信双方所遵循和协商好的规则就是协议。 网络层传输的包（packet）在数据链路层中传输的是帧就（frame，Rahmen）。数据包到达数据链路层后加上数据链路层的协议头和协议尾就构成了一个数据帧。 ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer/:2:0","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/computernetwork/data-link-layer/"},{"categories":["Notes"],"content":"MAC地址 是指局域网上的每一台计算机中固化在适配器的ROM中的地址（因此也叫适配器地址或适配器标识符EUI—48），由硬件厂商决定，是不会变的。只要适配器不变，它就不变。它是每一个站的“名字”或标识符。如果连接在局域网上的主机或路由器安装有多个适配器，那么这样的主机或路由器就有多个“地址”，也就是说，这种48位地址应当是某个接口的标识符。当然通过相应软件也是可以改变mac地址的。 MAC帧格式 ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer/:2:1","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/computernetwork/data-link-layer/"},{"categories":["Notes"],"content":"差错检测 ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer/:3:0","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/computernetwork/data-link-layer/"},{"categories":["Notes"],"content":"CRC亢余检测 ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer/:3:1","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/computernetwork/data-link-layer/"},{"categories":["Notes"],"content":"有穷自动机中的部分java实现","date":"2020-05-12","objectID":"/posts/theo/something-about-regular-expression-implementation/","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言---实现篇","uri":"/posts/theo/something-about-regular-expression-implementation/"},{"categories":["Notes"],"content":"本篇可以看作是“从有穷自动机到正则表达式”此篇的番外篇，主要是提供JAVA实现从DFA到RE的转换。因为代码完全是从作业中copy的，不知道会不会有版权问题，但这也不算商业用途，其中有一部分代码也是我自己写的，所以应该问题不大吧。（作业默认会给出Java和Haskell的一些源码，我不太想写Haskell所以就用Java来实现了，有兴趣的可以自己从copy代码后试一试） 先来看看正则表达式的实现，其中有一个Regex的抽象类，和另一篇博文中关于正则表达式的定义相对应，有6个子类来作为归纳的基础，分别是$\\epsilon , \\emptyset$, 单个字母，连接，并，星号，下面是代码。 import java.util.Arrays; import java.util.Collections; import java.util.HashMap; import java.util.HashSet; import java.util.Iterator; import java.util.LinkedList; import java.util.List; import java.util.Map; import java.util.Set; import java.util.stream.Stream; public abstract class Regex { /** * Determines whether the given string matches the regular expression. * @param word the string * @return */ public abstract boolean matches(String word); private static final class ReadOnlyIterator\u003cT\u003e implements Iterator\u003cT\u003e { private final Iterator\u003cT\u003e it; public ReadOnlyIterator(Iterator\u003cT\u003e it) { this.it = it; } @Override public boolean hasNext() { return it.hasNext(); } @Override public T next() { return it.next(); } } public static final class Empty extends Regex { @Override public boolean matches(String word) { return false; } @Override protected void appendTo(StringBuilder sb, int prec) { sb.append(\"{}\"); } private Empty() { } @Override protected int getPrecedence() { return 4; } @Override public int hashCode() { return 23; } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; return (getClass() == obj.getClass()); } public int size() { return 1; } @Override public boolean isNullable() { return false; } @Override public \u003cR\u003e R accept(Visitor\u003cR\u003e vis) { return vis.visitEmpty(); } } public static final class Epsilon extends Regex { @Override public boolean matches(String word) { return word.isEmpty(); } private Epsilon() { } @Override protected void appendTo(StringBuilder sb, int prec) { sb.append(\"()\"); } @Override protected int getPrecedence() { return 4; } @Override public int hashCode() { return 47; } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; return (getClass() == obj.getClass()); } public int size() { return 1; } @Override public boolean isNullable() { return true; } @Override public \u003cR\u003e R accept(Visitor\u003cR\u003e vis) { return vis.visitEpsilon(); } } public static final class Single extends Regex { private final char c; @Override public boolean matches(String word) { return word.length() == 1 \u0026\u0026 word.charAt(0) == c; } @Override protected int getPrecedence() { return 4; } public char getChar() { return c; } private Single(char c) { super(); this.c = c; } @Override protected void appendTo(StringBuilder sb, int prec) { sb.append(c); } @Override public int hashCode() { final int prime = 31; int result = 1; result = prime * result + c; return result; } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Single other = (Single) obj; if (c != other.c) return false; return true; } public int size() { return 1; } @Override public boolean isNullable() { return false; } @Override public \u003cR\u003e R accept(Visitor\u003cR\u003e vis) { return vis.visitSingle(c); } } public static final class Concat extends Regex implements Iterable\u003cRegex\u003e { private final List\u003cRegex\u003e children; private final int size; private final boolean nullable; private Concat(List\u003cRegex\u003e children) { super(); List\u003cRegex\u003e children2 = new LinkedList\u003c\u003e(); int size = 1; boolean nullable = true; for (Regex r : children) { if (r instanceof Epsilon) continue; children2.add(r); size += r.size(); nullable = nullable \u0026\u0026 r.isNullable(); } this.size = size; this.nullable = nullable; this.children = Collections.unmodifiableList(children2); } @Override public boolean matches(String word) { for (int i = 0; i \u003c= word.length(); ++i) { Regex concatRest = Regex.epsilon().concat(children.subList(1, children.size())); if (children.get(0).matches(word.substring(0, i)) \u0026\u0026 concatRest.matches(word.substring(i)) )","date":"2020-05-12","objectID":"/posts/theo/something-about-regular-expression-implementation/:0:0","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言---实现篇","uri":"/posts/theo/something-about-regular-expression-implementation/"},{"categories":["Notes"],"content":"初识高性能计算","date":"2020-05-03","objectID":"/posts/seminar-hpc/shared-memory-parallelism_1/","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--前导篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_1/"},{"categories":["Notes"],"content":"在目前正在更的Theo和Network系列上又要开启新的系列了呢，也就是目前看到的 parallel computing 系列。因为这个学期刚好选到一个主题是high performance computing (HPC) 的Seminar，然后自己被分配到的题目就是标题所说的\"shared memory parallelization\"。因为最后需要交一篇paper还要做一个presentation，觉得也可以新开一个系列来整理记录自己看各种资料时候学习到的相关的新知识，当然因为看的资料本身是英语，为了之后写paper方便我有些时候也就直接写英语了。本篇主要内容是最近要交的outline和一些关于并行计算的导引。 General concept why parallel? what is the idea of shared memory? memory modules Hardware views Different architecture UMA Systems Pro Nach NUMA Systems Pro Nach How do interconnection networks work? basic ideas that distiguish from normal networks switch, crossbar…. Cache issues Cache coherence: Snooping cache coherence Directory-based cache coherence False sharing: Basic explanaton, will have further discussion in programmer’s view Programmer’s view parallel programming models shard memory without threads threads model: some basic concept and implementation with OpenMP or CUDA some example parallel programs written in OpenMP that may concerning to solving flase sharing Difference in HPC Distiguish what’s special about shared memory in HPC (still not quite clear, need further readings) Materials related: Prgramming on Parallel Machines, Norm Matloff, Univercity of California, Davis An Introduction to Parallel Programming, Peter Pacheco Parallel Computer Architecture A Hardware / Software Approach, David Culler, Jaswinder Paul Singh Parallel Programming for Modern High Performance Computing Systems, Paul Czarnul Slide of an old lecture from TUM: https://www5.in.tum.de/lehre/vorlesungen/parhpp/SS08/ ","date":"2020-05-03","objectID":"/posts/seminar-hpc/shared-memory-parallelism_1/:0:0","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--前导篇","uri":"/posts/seminar-hpc/shared-memory-parallelism_1/"},{"categories":["Notes"],"content":"正则语言笔记","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"在上篇中我们了解到DFA与NFA的故事，从中我们初识了正则运算并且了解到其封闭性(虽然我还没有写完)，在本篇中我们将会里了解到正则表达式，进而到正则语言，我们可以从这个小小的角度对归纳的魅力有惊鸿一瞥，还可以了解到有穷自动机，正则表达式，正则语言它们之间的转换。 ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:0","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"正则表达式及形式化定义 正则表达式其实是正则语言(formal language)的另一种定义。 我们不妨来看看定义，这里其实就可以就感受到归纳的威力了！ R是正则表达式(RE)当且仅当R是 a, $a \\in \\Sigma$; /表示 {a}/ $\\epsilon$ /表示 ${\\epsilon}$/ $\\emptyset$ $(R_1 \\cup R_2)，R_1和 R_2都是正则表达式；$ $(R_1 R_2), R_1和R_2都是正则表达式；$ $(R_1^ *)，R_1是正则表达式.$ L(R): R表示的语言 ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:1","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"正则语言封闭性 有空再写吧，本质还是构造法 ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:2","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"正则表达式与有穷自动机的等价性 我们已经知道有穷自动机识别的语言是正则语言，而后我们又认识到正则表达式(regular expression，RE)也可以来定义语言，他们两者的关系是怎样的呢？下面我们就来证明正则表达和有穷自动机是等价的。 等价，参考上一篇文章也就是说他们能够描述同样的语言。 我们要证明的也就是以下定理： Satz 3.19 (Kleene 1956) Eine Sprache L $\\in$ $\\Sigma^{*}$ ist genau dann durch einen regulären Ausdruck darstellbar, wenn sie regulär ist. 这也其实就是要我们证明：一个语言是正则的当且仅当可用正则表达式描述这个语言。 我们要做的其实也就是有一个正则表达式，我们要构造一个有穷自动机识别正则表达式产生的那些串；然后给定一个自动机，我们要构造一个正则表达式使得这个RE描述的串正好是自动机识别的串，所以我们要给定一个算法能在有穷自动机和相应的RE之间进行转换，并且这种转换是等价的。 先来看看 “$\\implies$” 方向： 这里需要做的其实是就是构造一个有穷自动机去识别相应的语言，也就是将一个正则表达式转换为自动机。关于自动机其实为了方便只需要关注NFA就可以了，因为上一篇文章中我们已经证明了DFA和NFA的等价性。 正则表达式是通过前文说的6个基本步骤来定义的，前三个是基本步骤，后三个是归纳步骤；我们只需说明对于每一个步骤如何去构造相应的有穷自动机即可完成证明。 R = a, a$\\in\\Sigma$. L(R) = {a}, N = ({$q_1$, $q_2$ }, $\\Sigma$, $\\delta$, $q_1$, {$q_2$} ), $\\delta (q_1, a)$ = ${q_2 }$, $\\delta (r, b) = \\emptyset$, 若$r\\neq q_1$或$b \\neq a$ (其实就是只有a的语言) R = $\\epsilon$. L(R) = {$\\epsilon$}, N = ( {$ q_1 $}, $\\Sigma, \\delta, q_1$, {$q_1$} ), $\\forall r, \\forall b, \\delta (r, b) = \\emptyset$. (只有一个状态$q_1$接受空串，其他串都不接受) R = $\\emptyset$. L(R) = $\\emptyset$, N = ( {$q_1 $}, $\\Sigma , \\delta , q_1, \\emptyset$), $\\forall r, \\forall b, \\delta(r,b)=\\emptyset$. (只有一个状态$q_1$，但区别不是接受状态，也即什么串也不接受) $R=(R_1 \\cup R_2)$, 这里直接上教授slides上面的图 并/alternative 这个新自动机的接受状态为原来接收状态的并，并且用一个新的初始状态来代替旧的。 $R=(R_1 R_2)$, 这里还是看图说话 连接/concatenation 这个新自动机接受状态为后一个的接受状态，而初始状态只有第一个的了。 $ R =(R_1 ^*)$, 还是直接给出图比较直观 星号/star 加入一个新的初始状态，并且本身是一个接收状态，这样可以保证接受$\\epsilon$空串，然后通过$\\epsilon$连接新旧初始状态，和接受状态。 这一个方向证毕。 再来看看 “$\\Longleftarrow$” 方向： 这里其实有两种证明方式，一种是通过定于广义非确定型有穷自动机来证明，还有一种是Tobia教授课上给出的一种算法，我们不妨都来学习学习。 先来看看Tobias教授给出的算法： Sei $M = (Q, \\Sigma ,\\delta , q_1 , F) ein DFA$. (Funktioniret analog auch für NFA) WIr konstruieren einen RE $\\gamma$ mit L(M) = L ($\\gamma $). Sei Q = {$q_1 ,…,q_n$}.Wir setzen $$ R_{ij}^{k}:= \\lbrace w \\in \\Sigma ^* | die \\ Eingabe\\ w \\ führt\\ von\\ q_i\\ in\\ q_j ,wobei\\ alle Zwischenzustände (ohne\\ ersten\\ und\\ letzten) einen\\ Index\\ \\leq k\\ haben\\ \\rbrace $$ Behauptung: Für alle $i, j \\in \\lbrace1,…,n\\rbrace und\\ k \\in \\lbrace 0,…,n \\rbrace können\\ wir\\ einen\\ RE\\ L(\\alpha {ij}^k) = R{ij}^k$ Induktion über k: k = 0: Hier gilt 再来看看另一种证法： 其实也就是定义一个广义非确定型有穷自动机（GNFA），从DFA构造等价的GFNA，从GFNA构造等价的正则表达式。 GFNA的特殊在于状态间的转移不 ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:3","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"泵引理 泵引理（Pumping Lemma für reguläre Sprachen），用来证明一个语言不是正则的神奇工具。有些语言看起来很规则，但是却不是正则语言，因为无法用有穷自动机来描述，我们用泵引理即可很有效地判断不是正则语言的情况，但却无法证明是正则语言，因为只是必要条件而不是充分条件。下面来看看泵引理具体长什么样子，既有中文版，也有德语版： 泵引理：设A是正则语言，则存在常数p（称为泵长度），使得若 s $\\in$ A且 |s| $\\geq$，则 s = xyz，并且满足下述条件： $\\forall i \\geq 0, xy^iz \\in A;$ |y| \u003e 0; |xy| $\\leq p$. 这里第一个条件其实很有意思，这其实很像前面广义非确定有穷自动消除一个顶点后出来的表达式，实际上并不是巧合，从某个状态出发读了x之后进入一个状态，这个状态自己到自己有一个箭头，并且可以通过 y 来到达，这个状态读取 z后到接受状态（这个样子其实和水泵也很像）。 接下来看看德语版： Satz 3.30 (Pumping Lemma für reguläre Sprachen) Sei R $\\subseteq \\Sigma ^*$ regulär. Dann gibt es ein n \u003e 0, so dass sich jedes z $\\in$ R mit |z| $\\geq$ n so in $z = uvw$ zerlegen lässt, dass $v \\neq \\epsilon$ $|uv| \\leq n$, und $\\forall i \\geq 0. uv^iw \\in R$. 泵引理来证明语言不是正则的其实就是要来用上面的条件来推矛盾，有时候就需要灵感来取一个特殊的s，经过泵的步骤后就不在这个语言中了，就有了矛盾。 泵引理的证明 在这个证明中我们会明白如何取这个常数 p，为什么s会被分为三段，为什么xy的长度要小于等于p。 提前剧透：这里的p其实就是一个相应自动机的状态数。 证明：设A = L(M), M = (Q, $\\Sigma ，\\delta ，q_1, F$), |Q| = p, $s = s_1s_2…s_n \\in A, n \\geq p$. 设M在s上计算 $r_1,r_2,…,r_{n+1}, \\delta({r_i, s_i}) = r_{i+1}$ $r_i$为n+1个状态，经由s的n个输入字符可到达。由于总共只有p个状态，并且不超过n，而现在有n+1个状态，每一个都是Q中的元素，由著名的抽屉原理：这n+1个状态中最多只有p种不同的状态, 也就是说至少会有两个重复的状态。 根据抽屉原理，存在 j \u003c l, 使得 $r_j = r_l , l \\leq p + 1$. 令 $x = s_1 …s_{j - 1}, y=s_j …s_{l - 1}, z=s_l …s_{n+1}$. 由于$x让M从r_1 到r_j，y让M从r_j到 r_j, z让M从r_j到r_{n+1}, 而r_{n+1}是接受状态，所以\\forall i \\geq 0, xy^iz \\in A.$ 由于$j \\neq l , 所以|y|\u003e0$. 由于$l \\leq p+1, 所以|xy| \\leq p$. 这样就完成了泵引理的证明。 Tobia教授slides上面的证明也很类似： Sei R = L(A), A = ($Q, \\Sigma , \\delta , q_0 , F$). Sei n = |Q|. Sei nun z = $a_1a_2a_3…a_m \\in R$ mit m $\\geq n$. Die beim Lesen von $z$ durchlaufene Zustandfolge sei $q_0 = p_0 \\rightarrow ^{a_1} p_1 \\rightarrow ^{a_2}p_2 … \\rightarrow ^{a_m} p_m$ Dann muss es $0 \\leq i \\leq j \\leq n geben mit p_i = p_j$. Wir teilen $z$ wie folg auf: $$ \\underbrace{a_1…a_i}u\\underbrace{a{i+1}…a_j}v \\underbrace{a{j+1}…a_m}_w $$ Damit gilt: $|uv| \\leq n$ $v \\neq \\epsilon$, und $\\forall l \\geq 0 . uv^l w \\in R$ ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:4","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"Ardens Lemma Ardens Lemma, 也叫 Arden’s rule, 用来构造这样的自动机：只有一个初始状态，且其中没有$\\epsilon$移动。先来看看数学表达式长什么样，再来翻译翻译到底说了些啥。 $$ \\text{Sind A, B und X Sprachen mit} \\epsilon \\notin \\text{A, so gilt }\\ X = AX \\cup B \\Rightarrow X = A^B \\ \\text{其中还有一个类似的推论：} Sind\\ \\alpha , \\beta \\text{ und X regulär Ausdrücke mit } \\epsilon \\notin L(\\alpha), so\\ gilt \\ X \\equiv \\alpha X | \\beta \\Rightarrow X \\equiv \\alpha ^ \\beta $$ 于是有了这些的结论： ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:5","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"探索计算机网络OSI模型中的物理层的故事.","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer/","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/computernetwork/physical-layer/"},{"categories":["Notes"],"content":"探索计算机网络OSI模型中的物理层的故事. 现在我们来到了计算机网络的OSI模型的第一层：物理层(physical layer)。在这里我会以教授的slides为框架简要的讨论物理层中发生的故事,主要会关注从数字信号与模拟信号之间的转化过程，这是属于离散和连续的较量呢。之所以说是充满机遇的是因为离散与连续之间有着隐藏法术等待我们去习得。 ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer/:0:0","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/computernetwork/physical-layer/"},{"categories":["Notes"],"content":"信号和信息 要想理解物理层中发生的种种有意思的事情(当然主要是数学故事啦)，还是得要有些准备工作要做的。 我们不妨看看物理层到底干了些什么： 物理层利用传输介质为通信的两端建立、管理和释放物理链接，实现比特流的透明传输，保证比特流正确的传输到对端。物理层中承载的是比特流单位是比特（bit）。我知道我已经说的这么清楚明白，你可能依然犹如漫步在云端，头重脚轻不明所以。那么我用一个生活中的例子来帮助你更好的理解。 例：比如你在和朋友聊天，说话内容需要由大脑编排好，然后将你要讲的内容送达到嘴巴，嘴巴通过发出声音让对方听到你说话的内容。并且你在讲话时也无需考虑声波会如何传送到对方的耳朵中的，并且你也看不到声波是如何传达到对方的耳朵中的。这里声波就可以理解为是‘比特流’，你的嘴巴就是提供了‘物理层’的服务。 嘴巴负责开始谈话（建立链接）、判断谈话的开始和结束（管理链接）、结束谈话（释放链接）。并且嘴巴也无需考虑声波是如何传达到对方的耳朵中，所以声波对于嘴巴是透明的。在网络中‘透明’=管理成本低。 我们知道通过信号的强弱变化可以传递出二进制编码，而这些编码被识别成不同的符号，通过这些符号也就可以传达出各种各样的信息。 在信息论中我们其实可以通过引入物理学中熵(entropy)的概念来量化信息，接收到消息中包含的信息量又称作信息熵。事实上，1948年Shannon就是这么做的，将热力学的熵引入信息学中， 于是我们可以这样来定义信息： 信息，一方面可以理解为能够预测信号变化的不确定性。于是一个字母表X中的x的信息内容取决于信息携带信号在观察时该x的出现概率，于是定义为： $$ I(x)=-\\log_{2}{p(x)} $$ 单位为bit。 信息熵于是可以由求期望的方式给出： $$ H(X)=\\sum_{x\\in X} p(x)I(x) = - \\sum_{x\\in X}p(x)\\log_{2}{p(x)} $$ ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer/:1:0","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/computernetwork/physical-layer/"},{"categories":["Notes"],"content":"信号处理（Signaldarstellung） 在物理层传输介质中传输的信号共分为两种，模拟信号、数字信号，下图给出了这两种信号的范例： 模拟限号和数字信号 由于计算机只能识别数字信号，但要在广域网中传播却又以模拟信号的形式进行(光纤的情况又有不同，这个就是后话了)，于是我们会设置一个调制解调器把数字信号转化为模拟信号以及反向的过程。 而接下来我们就来讨论这两过程的细节问题。 ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer/:2:0","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/computernetwork/physical-layer/"},{"categories":["Notes"],"content":"数学基础 开头就有说物理层的故事是属于离散和连续之间的较量，虽说离散和连续这两个相对的概念之间是只有较量，但是使用合适的方法这二者却又可以互相转换，这也就是我们要在这部分说明的神奇的数学法术。 当然如果你有足够的悟性就又可以领悟到这两个史诗级别的法术呢。 傅里叶级数（Fourier series） 在数学中，傅里叶级数能把任何周期函数或周期信号分解成一个(可能由无穷个元素组成的)简单振荡函数的集合，也即正弦和余弦函数，同时也是我们后面会说到的采样定理的核心内容。 我这里直接给出教授slides上面的定义： Ein periodisches Signal s(t) lässt sich als Summe gewichteter Sinus- und Kosinus-Schwingungen darstellen. Die so entstehende Reihenentwicklung von s(t) bezeichnet man als Fourierreihe: $$ s(t)=\\frac{a_0}{2} + \\sum_{k=1}^{\\infty} {a_k\\cos{k{\\omega}t} + b_k\\sin{k{\\omega}t}} $$ 这里的$a_0$是相对y轴的偏移量，同时$a_k$和$b_k$这两个系数可以由如下定义： $$ a_k = \\frac{2}{T}\\int_0^T {s(t)\\cos{k{\\omega}t}} {\\rm d}t $$ $$ b_k = \\frac{2}{T}\\int_0^T {s(t)\\sin{k{\\omega}t}} {\\rm d}t $$ 傅里叶变换(Fourier transform) 傅里叶变换是一种线性积分变换，用于信号在时空域和频域之间的变换。实际上借用维基百科的话来说傅里叶变换就像化学分析，确定物质的基本成分；信号来自自然界，也可以对其进行分析，确定其基本成分。 Die Fourier-Transformierte einer stetigen, integrierbaren Funktion s(t) ist gegeben als $$ s(t) \\longrightarrow S(f) = \\frac{1}{\\sqrt{2\\pi}}\\int_{t=-\\infty}^{\\infty} {s(t)(\\cos{2\\pi ft}-i\\sin{2\\pi ft})} {\\rm d}t $$ 其中$i=\\sqrt{-1}$ , 当然这里也可以写成指数形式，就不再赘述。 ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer/:2:1","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/computernetwork/physical-layer/"},{"categories":["Notes"],"content":"采样，重构和量化 有了前文的数学基础后，我们就可以开始学习信号处理过程中的采样(sampling, Abtastung)，重构(Rekonstruktion)，以及量化(Quantisierung)，从而达到让人激动的离散与连续之间的转化。更具体来说，采样(时域离散)和量化(值域离散)相结合可将模拟信号转换为数字信号，重构则可以认为是采样的逆过程。其中著名的\"Nyquist-Shannon sampling theorem\", 也即“奈奎斯特–香农采样定理”，的内容是连续信号与离散信号之间的一个基本桥梁，其实更像是对于转换的限制条件，这里在后面会更详细聊到。 采样（Abtastung） 我们这里先看看维基百科上是怎么说的： 在信号处理领域，采样是将信号从连续时间域上的模拟信号转换到离散时间域上的离散信号的过程，以采样器实现。通常采样与量化联合进行，模拟信号先由采样器按照一定时间间隔采样获得时间上离散的信号，再经模数转换器（ADC）在数值上也进行离散化，从而得到数值和时间上都离散的数字信号。 通过采样得到的信号，是连续信号（例如，现实生活中的表示压力或速度的信号）的离散形式。连续信号通常每隔一定的时间间隔被模数转换器（ADC）采样，当时时间点上的连续信号的值被表现为离散的，或量化的值。 这样得到的信号的离散形式常常给数据带来一些误差。误差主要来自于两个方面，与连续模拟信号频谱有关的采样频率，以及量化时所用的字长。采样频率指的是对连续信号采样的频度。它代表了离散信号在和时域和空间域上的精确度。字长（比特的数量）用来表示离散信号的值，它体现了信号的大小的精确性。 再来看看教授的slide上面怎么说的： Das Signal s(t) wird mittels des Einheitsimpulses (Dirac-Impulses) $\\sigma[t]$ in äquidistanten Abständen $T_a$ (Abtastintervall) für n $\\in$ Z abgetastet: $$ \\hat{x}=s(t) \\sum_{n=-\\infty}^{\\infty}\\sigma[t-nT_a]= \\begin{cases} 1, \u0026 t=nT_a \\newline 0, \u0026 sonst\\ \\end{cases} $$ Da $\\hat{s}(t)$ nur zu den Zeitpunkten nTa für ganzzahlige n von Null verschieden ist, vereinbaren wir die Schreibweise $\\hat{s}[n]$ für zeitdiskrete aber wertkontinuierliche Signale. Zeitkontinuierliches Signal und Abtastwerte 重构 在这个过程中数字信号被转换成模拟信号，就如同把采样的过程逆转一样，称作demodulation。在理想的系统上，每经过取样的固定时间而读取新的数据时，输出会即时改变到该强度。经过这样的即时转换，离散的信号本质上会有大量的高频率能量，出现与采样率的倍数相关的谐波。要消灭这些谐波并使信号流畅，信号必须通过一些模拟滤波器，压制任何在预期频域外的能量。 时域中的乘法对应于频域中的卷积： $$ s(t) \\delta [t -nT] \\rightarrow \\frac{1}{T}S(f)*\\delta[f - n/T] $$ Reconstruction 香农定理 香农定理给出了信道通信传送速率的上限和信噪比以及带宽的关系。 Abtasttheorem von Shannon und Nyquist Ein auf |f | $\\leq$ B bandbegrenztes Signal s(t) ist \u003evollständig durch äquidistante Abtastwerte ˆ s[n] beschrieben, sofern diese nicht weiter als $T_a \\leq$1/2B auseinander liegen. Die Abtastfrequenz, welche eine vollständige Signalrekonstruktion \u003eerlaubt, ist folglich durch: $$ f_a \\geq 2B $$ nach unten beschränkt. 量化 ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer/:2:2","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/computernetwork/physical-layer/"},{"categories":["Notes"],"content":"传输信道 对于无噪声，M的通道，我们会有$M = 2^N$种可区分的符号，可实现的数据率如何变化呢？ 我们先来回顾一下熵： 假设信号源以相同的概率发射所有信号，这样信号源的熵（因而平均信息）最大。 对于宽度为B的信道上的传输速率，我们可以得到最大传输速率： Harleys Gesetz $C_H = 2B \\log_{2}(M) bit$ 同时还有新的定义：信号功率（Signallesitung） 信号振幅的平方的期望值与信号功率的平方相对应。方差（分散）信号的振幅对应于不含直流分量的信号功率，并代表信息承载量信号的功率。 回到正题，不妨想想在有 ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer/:3:0","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/computernetwork/physical-layer/"},{"categories":["Notes"],"content":"有穷自动机笔记1","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa/","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/theo/something-about-dfa-and-nfa/"},{"categories":["Notes"],"content":"对于DFA和NFA形式定义的一些理解，以及正则运算封闭性和DFA与NFA等价性的证明，也就是说DFA的故事是属于NFA的，是特例和常规的故事呢。 ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa/:0:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/theo/something-about-dfa-and-nfa/"},{"categories":["Notes"],"content":"对于确定型有穷自动机（DFA）的一些认识 对于确定型有穷自动机，也即deterministic finite automaton 或者说 deterministischer endlicher Automat，我们可以给出相应的形式定义，这里借用Tobias教授在课上给的定义： Ein deterministischer endlicher Automat (deterministic infinite automaton, DFA) M = (Q, ∑, δ, q0, F), besteht aus einer endlichen Menge von Zuständen Q, einem (endlichen) Eingabealphabet ∑, einer (totalen!) Übergangsfunktion δ: Q × ∑ → Q, einem Startzustand q0 ∈ Q, und einer Menge F ⊆ Q von Endzuständen (akzeptierenden Zust.) 也就是说我们首先有一个有穷状态集 Q 包括这个自动机的所有的状态(states)， 然后一个输入字母表 ∑，包括所有可能的输入，一个转移函数 δ，从现在的状态接受一个输入到一个新状态的映射(这里还有一个扩展的转移函数后面再说), 一个初始状态 q0 表示从哪开始，和这个自动机的接收状态的集合 F 作为 Q 的一个子集。 关于这个扩展转移函数我们可以做类似的定义： δ: Q × ∑* → Q 这里的∑*代表的是由∑构成的字符串，也就是说自动机也可以对字符串的输入进行反应，这也是为了方便而定义的。 该装置接受的语言记作： L(M):= {w∈∑*|δ(q0, w)∈F} 就是说从初始状态出发读完输入串w之后所处的状态是一个接受状态，那么就接受这个串。被DFA识别的语言也叫 正则语言。 ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa/:1:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/theo/something-about-dfa-and-nfa/"},{"categories":["Notes"],"content":"对于非确定型有穷自动机（NFA）的一些认识 对于非确定型有穷自动机(nondeterministic finite automaton 或者说 nichtdeterministischer endlicher Automat)与确定型的区别就在于非确定性：下一个状态可以不唯一确定，可以进行ε移动，多种选择(含0种选择)，我们还是来看看Tobias教授ppt上的定义： Ein nichtdeterministischer endlicher Automat (nondeterministic infinite automaton, NFA) ist ein 5-Tupel N = (Q, ∑, δ, q0, F), so dass Q, ∑, q0 und F sind wie bei einem DFA δ: Q × ∑ → P(Q) P(Q) = Menge aller Teilmengen von Q = 2^Q. Alternative: Relation δ ⊆ Q × ∑ × Q. 请注意这里的输入字母表 ∑ 是原本的 ∑和 ε的并, 也就还要加上长度为0的符号ε; 然后对于这里的转移函数 δ 在当前状态下读一个符号进入一个状态，因为不确定性有多种选择，所以进入多个状态，这里要用一个幂集来表示，也就是说这里的新状态是若干个Q的状态。同样这里对于转移函数 δ的扩展可以把 Q 改写为 P(Q),将原本的某一个状态扩展到多个状态的集合，这样就构成了多个备份。 该装置接受的语言记作： L(N):= {w∈∑*|δ({q0}, w) ∩ F ≠ Ø} 现在我们自然而然地会去想DFA与NFA之间有怎样的故事呢？DFA和NFA的能力是否一样？这也就是说它们是否识别同样的语言呢？ 我们知道DFA识别的语言NFA也可以识别，那么NFA识别的语言DFA是否也可以识别呢？这就涉及到它们的等价性的问题了。 ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa/:2:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/theo/something-about-dfa-and-nfa/"},{"categories":["Notes"],"content":"正则运算的封闭性 我们想要去证明DFA和NFA的等价性还是先证明正则运算的封闭性(有时间再写吧，主要还是构造法) ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa/:3:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/theo/something-about-dfa-and-nfa/"},{"categories":["Notes"],"content":"DFA与NFA的等价性 我们首先给出关于\"等价\"的定义，即两台机器识别同样的语言，它们的功能是一样的但是内部构造可能不一样；状态数，转移函数可能都不一样。于是我们接下来要证明的就是：每台NFA都有等价的DFA。(这里有一些题外话 如果是下推自动机或图灵机确定和非确定的故事就又需要我们去探索了) 这里我们就需要用到构造法来证明了hh 证明思路：对于给定的NFA，构造等价DFA，用DFA来模拟NFA，也即让DFA记住NFA的所有分支(理论上可行，因为NFA的k个状态是有穷的，于是所有可能的状态的子集合2^k个也是有穷的)，同时引入ε闭包的概念，对于每个状态子集合，经ε移动可达到的新状态子集合。 下面给出严格的证明： 设 NFA N = (Q, ∑, δ, q0, F),构造 DFA M = (Q', ∑, δ', q0', F'), L(M) = L(N). 令Q' = P(Q). 对R∈Q'和a∈∑, E(R)={q|从R出发沿0个或多个ε移动可达q}; δ'(R,a)=∪(r∈R)(R ∩ F ≠ Ø). ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa/:4:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/theo/something-about-dfa-and-nfa/"},{"categories":["Notes"],"content":"写在最后 这一周THEO的课重要的部分大概是这些了，还有一些基本概念就没有写上来，至于这些语言的概念请参考Chomsky hierarchy, 下周的课看ppt应该是和正则语言以及上下文无关文法有关，也挺期待的呢。 ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa/:5:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/theo/something-about-dfa-and-nfa/"}]