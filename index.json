[{"categories":["学习笔记"],"content":"游戏物理相关主题概况","date":"2020-10-30","objectID":"/posts/game_physics_1/","tags":["Physics in Games"],"title":"关于游戏中物理系统的一些小事 -- 1","uri":"/posts/game_physics_1/"},{"categories":["学习笔记"],"content":"假期过后又要开启新的系列了呢，这次带来的是有关游戏中的物理系统的呢。如果说在computer graphic中我们学习到的更多是有关空间类的法术，那么在游戏物理中我们将会领略到「模拟类法术」的魅力。不言自明的是在「模拟类法术」下也有很多不同的分支，我们将从「物理模拟」以及「游戏中的物理系统」的角度来认识。 在这个过程中，我们将会获得对真实现象进行物理模拟的「法术」知识，了解所讨论的物理模型并通过相应的numerical approaches来求解他们。我以为能够用编程语言来实现其中的各种模拟算法和integration methods才可以说是有学到「法术」，当然我这里会分享一些C++的实现，主要会关注以下领域的概念和方法：刚体建模，碰撞检测方法，弹簧和可形变物体，流体建模及模拟，还有numerical simulation基础，以及粒子系统（partical systems）。 ","date":"2020-10-30","objectID":"/posts/game_physics_1/:0:0","tags":["Physics in Games"],"title":"关于游戏中物理系统的一些小事 -- 1","uri":"/posts/game_physics_1/"},{"categories":null,"content":"Test test ","date":"2020-10-29","objectID":"/posts/first_post/:0:0","tags":null,"title":"First_post","uri":"/posts/first_post/"},{"categories":["学习笔记"],"content":"first draft","date":"2020-06-07","objectID":"/posts/shared-memory-parallelism_2/","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/shared-memory-parallelism_2/"},{"categories":["学习笔记"],"content":"这次带来的是提升篇，其实就是seminar要交的paper的first draft，倒也算是偷懒的一种方式吧。 ","date":"2020-06-07","objectID":"/posts/shared-memory-parallelism_2/:0:0","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/shared-memory-parallelism_2/"},{"categories":["学习笔记"],"content":"A brief Overview on Shared Memory Parallelism in Parallel Computing ","date":"2020-06-07","objectID":"/posts/shared-memory-parallelism_2/:1:0","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/shared-memory-parallelism_2/"},{"categories":["学习笔记"],"content":"1. Why parallel? To speed up, while we are facing the limitation of current transistors and increasing energy consumption. Now that we know it is necessary and lots of privilege besides you need to reconstruct your program yourself rather than automatically distributed by APIs in a serial way. So, it then leads to the following question: how do we write parallel programs? Truth be told, there are number of possible answers to this question, while most of them share the idea of partitioning the work among cores. The two commonly used approach for this: task-parallelism and data-parallelism. In task-parallelism, we partition the problems into separately tasks that will be carried out in cores. While in data-parallelism each core carries out roughly similar operations on its part of data. ","date":"2020-06-07","objectID":"/posts/shared-memory-parallelism_2/:1:1","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/shared-memory-parallelism_2/"},{"categories":["学习笔记"],"content":"2. As it was mentioned above, when we write programs that are explicitly parallel, we will be focusing on two major types of parallel systems: shared-memory and distributed-memory. What is the idea of shared memory system? In a shared memory system, processors are connected via an interconnection network, so that every core can access ach memory location. And there are different hardware structures in implementing this idea. In shared-memory system all processors are either connected directly to the main memory or have their own memory blocks and are accessible to each other through special hardware build into the processors. Anyway, shared memory parallel computers may have different implementations, but generally have in common the ability for all processors to access all memory as global address space, which means multiple processors can operate independently but share the same memory resources. Furthermore, changes in a memory location operated by one processor are visible to all other processors. Interconnection topologies Thus, the first type of system is called a uniform memory access, or UMA, system, while the second type is called a nonuniform memory access, or NUMA, system. In UMA systems the time for every core to access the memory locations is the same, while in NUMA access time from one cache to distributed data parts varies as topology place. The NUMA Systems In a Nonuniform Memory Access architecture, each CPU has a memory module physically next to it, Advantages: Global address space provides a user-friendly programming perspective to memory Data sharing between tasks is both fast and uniform due to the proximity of memory to CPUs Disadvantages: Primary disadvantage is the lack of scalability between memory and CPUs. Programmer responsibility for synchronization constructs that ensure “correct” access of global memory. Interconnection networks This is important in implementing hardware of shared-memory parallelism: the efficiency of data exchange between memory and processors have huge impact on final execution time. Shared-memory interconnects Here we will explain two most widely used interconnects in shared-memory systems: buses and crossbars. The key property for a bus is that the devices connected to it share the communication wires. Since the amount of our cores is not many, it is more flexible to use a bus. However, the expected performance decreases as the number of cores connected to the bus increases. Thus, buses are replaced by switched interconnects in a more complicated shared-memory system. As name suggests, switched interconnects use switches to control the data among the connected devices. (Figure of crossbars) The individual switches can be shown as one from following figure (Figure of crossbar status) Crossbar switches are too expensive for large -scale systems, but are useful in some small systems. However, we can simplify the idea, that leads us to Omega (or Delta) Interconnects, which is similar to crossbars, but with fewer paths. (Figure of Omega Interconnects) Cache Issues Before we go further on this topic, let us firstly recall the idea of caching. To solve the problem of redundant time of processors accessing data in main memory we added block of Cache Coherency Explanation: To understand these issues, suppose we have a shared-memory system with two cores, each of which has its own private data cache. As long as the two cores only read share data, there is no problem. For example, suppose that x is a shared variable that has been initialized to value 3. In one cache we have the instruction that changes the value of x to 7, while in another cache at the same time also have operation concerning the value of x, which leads to the problem of 3 or 7? (could be a figure involving things describes above) However, this will be unpredictable situation when situations mentioned above occur regardless of whether the system using which policy among processors, because this occurs in caches with in ","date":"2020-06-07","objectID":"/posts/shared-memory-parallelism_2/:1:2","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/shared-memory-parallelism_2/"},{"categories":["学习笔记"],"content":"3. OpenMP is an API designed for programming shared-memory parallel programming. The MP in OpenMP stands for “multiprocessing”. Something good about OpenMP is that the programmer does not need to specify how each thread behave explicitly, which suggests that OpenMP allows the programmer to simply mark that the block of code should be executed in parallel, and the exact determination of which thread should execute them is handed to the compiler and the run-time system. In other word, OpenMP requires more compiler support rather works like a library of functions. This convenience in writing parallel program does not come at no cost: we give away the power to program virtually any possible thread behaviour in exchange. (give an example of a program in OpenMP, then introduce the basic rules of writing program with OpenMP) ","date":"2020-06-07","objectID":"/posts/shared-memory-parallelism_2/:1:3","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/shared-memory-parallelism_2/"},{"categories":["学习笔记"],"content":"4.Usage in high performance computing Parallelism not always make the execution faster, sometimes the more parallelism we had, the slower the program ran, which means we need to reconsider the task before we choose to implement it parallelly. ​ (table with run time of Dijkstra with 1000 nodes but different number of threads) ​ (table with run time of Dijkstra with 25000 nodes but different number of threads) Possible way to improve our performance when writing parallel program with OpenMP on shared-memory systems: False sharing could be a problem. Example to this: analysis the execution of a Matrix multiplies a vector. $Y = A * x$ #pragma omp parallel for num_threads(thread_count) \\ default(none) private (i, j) shared(A, x, y, m, n) for(i =0; i\u003c m; i++){ y[i] = 0.0; for(j = 0; j \u003c n; j++){ y[i] += A[i][j] * x[j]; } } We will compare the performance of the matrix 8000000 x8 or 8000000 x 8 or 8 x 8000000 (efficiency table here) Although we may face much larger numbers in high performance computing Why is multi threads worse than less threads? Suppose for the moment that threads 0 and 1 are assigned to one of the processors and threads 2 and 3 are assigned to the other. Also suppose that for the 8x8,000,000 problem all of y is stored in a single cache line. Then every write to some element of y will invalidate the line in the other processor’s cache. For example, each time thread 0 updates y[0] in the statement. $y[i] += A[i][j] * x[j];$ if thread 2 or 3 is executing this code, it will have to reload y. Each thread will update each of its components 8,000,000 times. We see that with this assignment of threads to processors and components of y to cache lines, all the threads will have to reload y many times. This is going to happen in spite of the fact that only one thread accesses any one component of y, for example, only thread0 accesses $y[0]$. (Explain why false sharing may not be a problem here) Possible ways of avoiding false sharing in the matrix multiplication program: One possible solution is to “pad” y vector with dummy elements in order to guarantee that any update by one thread won’t influence another cache line. Another alternative way is to have its own private storage during the loop, and then update the shared storage when iterations are done. ","date":"2020-06-07","objectID":"/posts/shared-memory-parallelism_2/:1:4","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--提升篇","uri":"/posts/shared-memory-parallelism_2/"},{"categories":["学习笔记"],"content":"数据链路层介绍","date":"2020-05-12","objectID":"/posts/data-link-layer/","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/data-link-layer/"},{"categories":["学习笔记"],"content":"连接的特征（Verbindungscharakterisierung） 两个节点之间的连接会有如下的各种属性：传输速率（Übertragungsrate），传输延迟（Übertragungsverzögerung），传输方向（Übertragungsrichtung），以及多路访问或多路复用（Mehrfachzugriff (Multiplexing)）。 ","date":"2020-05-12","objectID":"/posts/data-link-layer/:1:0","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/data-link-layer/"},{"categories":["学习笔记"],"content":"点对点的信道 Übertragungsrate 数据在信道上面被放置的时间我们称作Serialisierungszeit，记作 $t_s$, L 为需要传输的数据大小，r 为传输速率，于是 $$ t_s = \\frac{L}{r} $$ 这里的$t_s$可以认为是传输时延，传输这些数据会要用到的时间。 Ausbreitungsverzögerung 顾名思义，传播延迟，与上面的不一样的是这是信号在信道中从一段到另一端会花的时间： $$ t_p = \\frac{d}{vc_0} $$ $c_0$即为光速，v为一个常数， 与传输介质有关。 Gesamtverzögerung $t_d 即是总的延迟为上面两项的和$。 Bandbreitenverzögerungsprodukt 这个就是所谓的宽带延迟，正是因为在信道中的传播不可避免的需要时间，我们需要一定容量来存储： $$ C = t_p r = \\frac{d}{vc_0} r $$ 单位是bit。 Übertragungsrichtung Übertragungsrichtung ","date":"2020-05-12","objectID":"/posts/data-link-layer/:1:1","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/data-link-layer/"},{"categories":["学习笔记"],"content":"广播信道的数据链路层 一般来说这里我们会用到时分复用（Zeitmultiplex），即用一个信道实现不同端口之间的通信，基于分组网络（以太网，无线局域网）中的非确定性方法（并发访问）。 至于复用我想教授的slide上面的图已经很清楚了： Multiplexing ALOHA and Sorted ALOHA CSMA, CSMA/CD, CSMA/CA CSMA 指的其实是 Carrier Sense Multiple Access, 是对于前面sorted ALOHA的一种简单的优化，也即“listen before talk”。 CSMA/CD 中文翻译也即 载波监听碰撞监听多点接入/碰撞检测，以太网即用此协议，分为载波监听和碰撞检测两部分：即“发送前先监听”，每个站在发送数据前要先检测一下总线上是否有其他站在发送数据，如果有，暂时不发送数据，等待信道空闲时再发送，总线上没有“载波”，这里只是一个习惯称呼；-碰撞检测：即“边发送边监听”，适配器边发送数据边检测信道上的信号电压，以便判断自己在发送数据时其他站是否也在发送数据。同时发送数据时，总线上的信号电压变化幅度大，超过一定门限值时，就认为总线上至少两个站同时在发送数据，表明有碰撞。这时总线上的信号失真，无法恢复。所以，每一个正在发送数据的站，一旦发现有碰撞，适配器就要立即停止发送，以免浪费网络资源，等待一段随机时间后再发送。 如果考虑上信号在链路上的传播时延，那么过程类似这样； CSMA/CD 每个端点在自己发送数据之后的一小短时间内，存在着遭遇碰撞的可能性，这段时间最长为两个单程最长时间，将这个时间成为“争用期”，只有通过争用期的“考验”，才能肯定这次发送不会发生碰撞。正是因为这个原因，以太网规定了数据帧的最小长度即64字节，所有小于此长度都认为是碰撞导致的丢弃帧。当我们接收到至少64字节我们就可认为这之间没有碰撞。 CSMA/CD不能同时进行发送和接受，因此是Halbdulplex，也就是半双工协议，即双向交替通信。 CSMA/CA 在有线连接的局域网中就不能使用CSMA/CD（无线局域网中），因为即使发送的信息足够长, 也不能总是检测到碰撞。这里的CA其实就是colision avoidance，发送包的同时不能检测到信道上有无冲突，只能尽量“避免”。例如，如果计算机A和计算机C同时给计算机B发送一个控制消息，它们将同时到达计算机B，导致冲突的发生。当这种冲突发生时，发送者可以随机等待一段时间，然后重发控制消息。因为控制消息比数据帧要短得多，所以发生第二次冲突的可能性也要比传统以太网要小很多。最终将有一个控制消息正确到达，然后计算机B发送一个响应消息。通常CSMA/CA利用ACK信号来避免冲突的发生，也就是说，只有当客户端收到网络上返回的ACK信号后才确认送出的数据已经正确到达目的。 教授slide中是这样说的： Wenn Medium frei, übertrage mit Wahrscheinlichkeit p oder verzögere mit Wahrscheinlichkeit 1 − p um eine feste Zeit dann 1. Wenn Medium belegt, warte bis frei, dann 1. ","date":"2020-05-12","objectID":"/posts/data-link-layer/:1:2","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/data-link-layer/"},{"categories":["学习笔记"],"content":"封装成帧 封装简单说就是事先对数据包进行拆分和打包，在所发送的数据包上附加上目标地址，本地地址，以及一些用于纠错的字节等。对数据包进行处理时通信双方所遵循和协商好的规则就是协议。 网络层传输的包（packet）在数据链路层中传输的是帧就（frame，Rahmen）。数据包到达数据链路层后加上数据链路层的协议头和协议尾就构成了一个数据帧。 ","date":"2020-05-12","objectID":"/posts/data-link-layer/:2:0","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/data-link-layer/"},{"categories":["学习笔记"],"content":"MAC地址 是指局域网上的每一台计算机中固化在适配器的ROM中的地址（因此也叫适配器地址或适配器标识符EUI—48），由硬件厂商决定，是不会变的。只要适配器不变，它就不变。它是每一个站的“名字”或标识符。如果连接在局域网上的主机或路由器安装有多个适配器，那么这样的主机或路由器就有多个“地址”，也就是说，这种48位地址应当是某个接口的标识符。当然通过相应软件也是可以改变mac地址的。 MAC帧格式 ","date":"2020-05-12","objectID":"/posts/data-link-layer/:2:1","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/data-link-layer/"},{"categories":["学习笔记"],"content":"差错检测 ","date":"2020-05-12","objectID":"/posts/data-link-layer/:3:0","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/data-link-layer/"},{"categories":["学习笔记"],"content":"CRC亢余检测 ","date":"2020-05-12","objectID":"/posts/data-link-layer/:3:1","tags":["computer network"],"title":"功能重要单一的数据链路层","uri":"/posts/data-link-layer/"},{"categories":["学习笔记"],"content":"初识高性能计算","date":"2020-05-03","objectID":"/posts/shared-memory-parallelism_1/","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--前导篇","uri":"/posts/shared-memory-parallelism_1/"},{"categories":["学习笔记"],"content":"在目前正在更的Theo和Network系列上又要开启新的系列了呢，也就是目前看到的 parallel computing 系列。因为这个学期刚好选到一个主题是high performance computing (HPC) 的Seminar，然后自己被分配到的题目就是标题所说的\"shared memory parallelization\"。因为最后需要交一篇paper还要做一个presentation，觉得也可以新开一个系列来整理记录自己看各种资料时候学习到的相关的新知识，当然因为看的资料本身是英语，为了之后写paper方便我有些时候也就直接写英语了。本篇主要内容是最近要交的outline和一些关于并行计算的导引。 General concept why parallel? what is the idea of shared memory? memory modules Hardware views Different architecture UMA Systems Pro Nach NUMA Systems Pro Nach How do interconnection networks work? basic ideas that distiguish from normal networks switch, crossbar…. Cache issues Cache coherence: Snooping cache coherence Directory-based cache coherence False sharing: Basic explanaton, will have further discussion in programmer’s view Programmer’s view parallel programming models shard memory without threads threads model: some basic concept and implementation with OpenMP or CUDA some example parallel programs written in OpenMP that may concerning to solving flase sharing Difference in HPC Distiguish what’s special about shared memory in HPC (still not quite clear, need further readings) Materials related: Prgramming on Parallel Machines, Norm Matloff, Univercity of California, Davis An Introduction to Parallel Programming, Peter Pacheco Parallel Computer Architecture A Hardware / Software Approach, David Culler, Jaswinder Paul Singh Parallel Programming for Modern High Performance Computing Systems, Paul Czarnul Slide of an old lecture from TUM: https://www5.in.tum.de/lehre/vorlesungen/parhpp/SS08/ ","date":"2020-05-03","objectID":"/posts/shared-memory-parallelism_1/:0:0","tags":["parallel computing","Seminar-HPC"],"title":"当我们谈论 shared memory parallelism 的时候我们在谈论些什么--前导篇","uri":"/posts/shared-memory-parallelism_1/"},{"categories":["学习笔记"],"content":"探索计算机网络OSI模型中的物理层的故事.","date":"2020-05-02","objectID":"/posts/physical-layer/","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/physical-layer/"},{"categories":["学习笔记"],"content":"探索计算机网络OSI模型中的物理层的故事. 现在我们来到了计算机网络的OSI模型的第一层：物理层(physical layer)。在这里我会以教授的slides为框架简要的讨论物理层中发生的故事,主要会关注从数字信号与模拟信号之间的转化过程，这是属于离散和连续的较量呢。之所以说是充满机遇的是因为离散与连续之间有着隐藏法术等待我们去习得。 ","date":"2020-05-02","objectID":"/posts/physical-layer/:0:0","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/physical-layer/"},{"categories":["学习笔记"],"content":"信号和信息 要想理解物理层中发生的种种有意思的事情(当然主要是数学故事啦)，还是得要有些准备工作要做的。 我们不妨看看物理层到底干了些什么： 物理层利用传输介质为通信的两端建立、管理和释放物理链接，实现比特流的透明传输，保证比特流正确的传输到对端。物理层中承载的是比特流单位是比特（bit）。我知道我已经说的这么清楚明白，你可能依然犹如漫步在云端，头重脚轻不明所以。那么我用一个生活中的例子来帮助你更好的理解。 例：比如你在和朋友聊天，说话内容需要由大脑编排好，然后将你要讲的内容送达到嘴巴，嘴巴通过发出声音让对方听到你说话的内容。并且你在讲话时也无需考虑声波会如何传送到对方的耳朵中的，并且你也看不到声波是如何传达到对方的耳朵中的。这里声波就可以理解为是‘比特流’，你的嘴巴就是提供了‘物理层’的服务。 嘴巴负责开始谈话（建立链接）、判断谈话的开始和结束（管理链接）、结束谈话（释放链接）。并且嘴巴也无需考虑声波是如何传达到对方的耳朵中，所以声波对于嘴巴是透明的。在网络中‘透明’=管理成本低。 我们知道通过信号的强弱变化可以传递出二进制编码，而这些编码被识别成不同的符号，通过这些符号也就可以传达出各种各样的信息。 在信息论中我们其实可以通过引入物理学中熵(entropy)的概念来量化信息，接收到消息中包含的信息量又称作信息熵。事实上，1948年Shannon就是这么做的，将热力学的熵引入信息学中， 于是我们可以这样来定义信息： 信息，一方面可以理解为能够预测信号变化的不确定性。于是一个字母表X中的x的信息内容取决于信息携带信号在观察时该x的出现概率，于是定义为： $$ I(x)=-\\log_{2}{p(x)} $$ 单位为bit。 信息熵于是可以由求期望的方式给出： $$ H(X)=\\sum_{x\\in X} p(x)I(x) = - \\sum_{x\\in X}p(x)\\log_{2}{p(x)} $$ ","date":"2020-05-02","objectID":"/posts/physical-layer/:1:0","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/physical-layer/"},{"categories":["学习笔记"],"content":"信号处理（Signaldarstellung） 在物理层传输介质中传输的信号共分为两种，模拟信号、数字信号，下图给出了这两种信号的范例： 模拟限号和数字信号 由于计算机只能识别数字信号，但要在广域网中传播却又以模拟信号的形式进行(光纤的情况又有不同，这个就是后话了)，于是我们会设置一个调制解调器把数字信号转化为模拟信号以及反向的过程。 而接下来我们就来讨论这两过程的细节问题。 ","date":"2020-05-02","objectID":"/posts/physical-layer/:2:0","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/physical-layer/"},{"categories":["学习笔记"],"content":"数学基础 开头就有说物理层的故事是属于离散和连续之间的较量，虽说离散和连续这两个相对的概念之间是只有较量，但是使用合适的方法这二者却又可以互相转换，这也就是我们要在这部分说明的神奇的数学法术。 当然如果你有足够的悟性就又可以领悟到这两个史诗级别的法术呢。 傅里叶级数（Fourier series） 在数学中，傅里叶级数能把任何周期函数或周期信号分解成一个(可能由无穷个元素组成的)简单振荡函数的集合，也即正弦和余弦函数，同时也是我们后面会说到的采样定理的核心内容。 我这里直接给出教授slides上面的定义： Ein periodisches Signal s(t) lässt sich als Summe gewichteter Sinus- und Kosinus-Schwingungen darstellen. Die so entstehende Reihenentwicklung von s(t) bezeichnet man als Fourierreihe: $$ s(t)=\\frac{a_0}{2} + \\sum_{k=1}^{\\infty} {a_k\\cos{k{\\omega}t} + b_k\\sin{k{\\omega}t}} $$ 这里的$a_0$是相对y轴的偏移量，同时$a_k$和$b_k$这两个系数可以由如下定义： $$ a_k = \\frac{2}{T}\\int_0^T {s(t)\\cos{k{\\omega}t}} {\\rm d}t $$ $$ b_k = \\frac{2}{T}\\int_0^T {s(t)\\sin{k{\\omega}t}} {\\rm d}t $$ 傅里叶变换(Fourier transform) 傅里叶变换是一种线性积分变换，用于信号在时空域和频域之间的变换。实际上借用维基百科的话来说傅里叶变换就像化学分析，确定物质的基本成分；信号来自自然界，也可以对其进行分析，确定其基本成分。 Die Fourier-Transformierte einer stetigen, integrierbaren Funktion s(t) ist gegeben als $$ s(t) \\longrightarrow S(f) = \\frac{1}{\\sqrt{2\\pi}}\\int_{t=-\\infty}^{\\infty} {s(t)(\\cos{2\\pi ft}-i\\sin{2\\pi ft})} {\\rm d}t $$ 其中$i=\\sqrt{-1}$ , 当然这里也可以写成指数形式，就不再赘述。 ","date":"2020-05-02","objectID":"/posts/physical-layer/:2:1","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/physical-layer/"},{"categories":["学习笔记"],"content":"采样，重构和量化 有了前文的数学基础后，我们就可以开始学习信号处理过程中的采样(sampling, Abtastung)，重构(Rekonstruktion)，以及量化(Quantisierung)，从而达到让人激动的离散与连续之间的转化。更具体来说，采样(时域离散)和量化(值域离散)相结合可将模拟信号转换为数字信号，重构则可以认为是采样的逆过程。其中著名的\"Nyquist-Shannon sampling theorem\", 也即“奈奎斯特–香农采样定理”，的内容是连续信号与离散信号之间的一个基本桥梁，其实更像是对于转换的限制条件，这里在后面会更详细聊到。 采样（Abtastung） 我们这里先看看维基百科上是怎么说的： 在信号处理领域，采样是将信号从连续时间域上的模拟信号转换到离散时间域上的离散信号的过程，以采样器实现。通常采样与量化联合进行，模拟信号先由采样器按照一定时间间隔采样获得时间上离散的信号，再经模数转换器（ADC）在数值上也进行离散化，从而得到数值和时间上都离散的数字信号。 通过采样得到的信号，是连续信号（例如，现实生活中的表示压力或速度的信号）的离散形式。连续信号通常每隔一定的时间间隔被模数转换器（ADC）采样，当时时间点上的连续信号的值被表现为离散的，或量化的值。 这样得到的信号的离散形式常常给数据带来一些误差。误差主要来自于两个方面，与连续模拟信号频谱有关的采样频率，以及量化时所用的字长。采样频率指的是对连续信号采样的频度。它代表了离散信号在和时域和空间域上的精确度。字长（比特的数量）用来表示离散信号的值，它体现了信号的大小的精确性。 再来看看教授的slide上面怎么说的： Das Signal s(t) wird mittels des Einheitsimpulses (Dirac-Impulses) $\\sigma[t]$ in äquidistanten Abständen $T_a$ (Abtastintervall) für n $\\in$ Z abgetastet: $$ \\hat{x}=s(t) \\sum_{n=-\\infty}^{\\infty}\\sigma[t-nT_a]= \\begin{cases} 1, \u0026 t=nT_a \\newline 0, \u0026 sonst\\ \\end{cases} $$ Da $\\hat{s}(t)$ nur zu den Zeitpunkten nTa für ganzzahlige n von Null verschieden ist, vereinbaren wir die Schreibweise $\\hat{s}[n]$ für zeitdiskrete aber wertkontinuierliche Signale. Zeitkontinuierliches Signal und Abtastwerte 重构 在这个过程中数字信号被转换成模拟信号，就如同把采样的过程逆转一样，称作demodulation。在理想的系统上，每经过取样的固定时间而读取新的数据时，输出会即时改变到该强度。经过这样的即时转换，离散的信号本质上会有大量的高频率能量，出现与采样率的倍数相关的谐波。要消灭这些谐波并使信号流畅，信号必须通过一些模拟滤波器，压制任何在预期频域外的能量。 时域中的乘法对应于频域中的卷积： $$ s(t) \\delta [t -nT] \\rightarrow \\frac{1}{T}S(f)*\\delta[f - n/T] $$ Reconstruction 香农定理 香农定理给出了信道通信传送速率的上限和信噪比以及带宽的关系。 Abtasttheorem von Shannon und Nyquist Ein auf |f | $\\leq$ B bandbegrenztes Signal s(t) ist \u003evollständig durch äquidistante Abtastwerte ˆ s[n] beschrieben, sofern diese nicht weiter als $T_a \\leq$1/2B auseinander liegen. Die Abtastfrequenz, welche eine vollständige Signalrekonstruktion \u003eerlaubt, ist folglich durch: $$ f_a \\geq 2B $$ nach unten beschränkt. 量化 ","date":"2020-05-02","objectID":"/posts/physical-layer/:2:2","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/physical-layer/"},{"categories":["学习笔记"],"content":"传输信道 对于无噪声，M的通道，我们会有$M = 2^N$种可区分的符号，可实现的数据率如何变化呢？ 我们先来回顾一下熵： 假设信号源以相同的概率发射所有信号，这样信号源的熵（因而平均信息）最大。 对于宽度为B的信道上的传输速率，我们可以得到最大传输速率： Harleys Gesetz $C_H = 2B \\log_{2}(M) bit$ 同时还有新的定义：信号功率（Signallesitung） 信号振幅的平方的期望值与信号功率的平方相对应。方差（分散）信号的振幅对应于不含直流分量的信号功率，并代表信息承载量信号的功率。 回到正题，不妨想想在有 ","date":"2020-05-02","objectID":"/posts/physical-layer/:3:0","tags":["computer network"],"title":"充满机遇的物理层","uri":"/posts/physical-layer/"},{"categories":["学习笔记"],"content":"有穷自动机笔记1","date":"2020-04-25","objectID":"/posts/something-about-dfa-and-nfa/","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/something-about-dfa-and-nfa/"},{"categories":["学习笔记"],"content":"对于DFA和NFA形式定义的一些理解，以及正则运算封闭性和DFA与NFA等价性的证明，也就是说DFA的故事是属于NFA的，是特例和常规的故事呢。 ","date":"2020-04-25","objectID":"/posts/something-about-dfa-and-nfa/:0:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/something-about-dfa-and-nfa/"},{"categories":["学习笔记"],"content":"对于确定型有穷自动机（DFA）的一些认识 对于确定型有穷自动机，也即deterministic finite automaton 或者说 deterministischer endlicher Automat，我们可以给出相应的形式定义，这里借用Tobias教授在课上给的定义： Ein deterministischer endlicher Automat (deterministic infinite automaton, DFA) M = (Q, ∑, δ, q0, F), besteht aus einer endlichen Menge von Zuständen Q, einem (endlichen) Eingabealphabet ∑, einer (totalen!) Übergangsfunktion δ: Q × ∑ → Q, einem Startzustand q0 ∈ Q, und einer Menge F ⊆ Q von Endzuständen (akzeptierenden Zust.) 也就是说我们首先有一个有穷状态集 Q 包括这个自动机的所有的状态(states)， 然后一个输入字母表 ∑，包括所有可能的输入，一个转移函数 δ，从现在的状态接受一个输入到一个新状态的映射(这里还有一个扩展的转移函数后面再说), 一个初始状态 q0 表示从哪开始，和这个自动机的接收状态的集合 F 作为 Q 的一个子集。 关于这个扩展转移函数我们可以做类似的定义： δ: Q × ∑* → Q 这里的∑*代表的是由∑构成的字符串，也就是说自动机也可以对字符串的输入进行反应，这也是为了方便而定义的。 该装置接受的语言记作： L(M):= {w∈∑*|δ(q0, w)∈F} 就是说从初始状态出发读完输入串w之后所处的状态是一个接受状态，那么就接受这个串。被DFA识别的语言也叫 正则语言。 ","date":"2020-04-25","objectID":"/posts/something-about-dfa-and-nfa/:1:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/something-about-dfa-and-nfa/"},{"categories":["学习笔记"],"content":"对于非确定型有穷自动机（NFA）的一些认识 对于非确定型有穷自动机(nondeterministic finite automaton 或者说 nichtdeterministischer endlicher Automat)与确定型的区别就在于非确定性：下一个状态可以不唯一确定，可以进行ε移动，多种选择(含0种选择)，我们还是来看看Tobias教授ppt上的定义： Ein nichtdeterministischer endlicher Automat (nondeterministic infinite automaton, NFA) ist ein 5-Tupel N = (Q, ∑, δ, q0, F), so dass Q, ∑, q0 und F sind wie bei einem DFA δ: Q × ∑ → P(Q) P(Q) = Menge aller Teilmengen von Q = 2^Q. Alternative: Relation δ ⊆ Q × ∑ × Q. 请注意这里的输入字母表 ∑ 是原本的 ∑和 ε的并, 也就还要加上长度为0的符号ε; 然后对于这里的转移函数 δ 在当前状态下读一个符号进入一个状态，因为不确定性有多种选择，所以进入多个状态，这里要用一个幂集来表示，也就是说这里的新状态是若干个Q的状态。同样这里对于转移函数 δ的扩展可以把 Q 改写为 P(Q),将原本的某一个状态扩展到多个状态的集合，这样就构成了多个备份。 该装置接受的语言记作： L(N):= {w∈∑*|δ({q0}, w) ∩ F ≠ Ø} 现在我们自然而然地会去想DFA与NFA之间有怎样的故事呢？DFA和NFA的能力是否一样？这也就是说它们是否识别同样的语言呢？ 我们知道DFA识别的语言NFA也可以识别，那么NFA识别的语言DFA是否也可以识别呢？这就涉及到它们的等价性的问题了。 ","date":"2020-04-25","objectID":"/posts/something-about-dfa-and-nfa/:2:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/something-about-dfa-and-nfa/"},{"categories":["学习笔记"],"content":"正则运算的封闭性 我们想要去证明DFA和NFA的等价性还是先证明正则运算的封闭性(有时间再写吧，主要还是构造法) ","date":"2020-04-25","objectID":"/posts/something-about-dfa-and-nfa/:3:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/something-about-dfa-and-nfa/"},{"categories":["学习笔记"],"content":"DFA与NFA的等价性 我们首先给出关于\"等价\"的定义，即两台机器识别同样的语言，它们的功能是一样的但是内部构造可能不一样；状态数，转移函数可能都不一样。于是我们接下来要证明的就是：每台NFA都有等价的DFA。(这里有一些题外话 如果是下推自动机或图灵机确定和非确定的故事就又需要我们去探索了) 这里我们就需要用到构造法来证明了hh 证明思路：对于给定的NFA，构造等价DFA，用DFA来模拟NFA，也即让DFA记住NFA的所有分支(理论上可行，因为NFA的k个状态是有穷的，于是所有可能的状态的子集合2^k个也是有穷的)，同时引入ε闭包的概念，对于每个状态子集合，经ε移动可达到的新状态子集合。 下面给出严格的证明： 设 NFA N = (Q, ∑, δ, q0, F),构造 DFA M = (Q', ∑, δ', q0', F'), L(M) = L(N). 令Q' = P(Q). 对R∈Q'和a∈∑, E(R)={q|从R出发沿0个或多个ε移动可达q}; δ'(R,a)=∪(r∈R)(R ∩ F ≠ Ø). ","date":"2020-04-25","objectID":"/posts/something-about-dfa-and-nfa/:4:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/something-about-dfa-and-nfa/"},{"categories":["学习笔记"],"content":"写在最后 这一周THEO的课重要的部分大概是这些了，还有一些基本概念就没有写上来，至于这些语言的概念请参考Chomsky hierarchy, 下周的课看ppt应该是和正则语言以及上下文无关文法有关，也挺期待的呢。 ","date":"2020-04-25","objectID":"/posts/something-about-dfa-and-nfa/:5:0","tags":["theory of computation","theoretical computer science"],"title":"DFA与NFA的故事(一)","uri":"/posts/something-about-dfa-and-nfa/"}]