[{"categories":["Notes"],"content":"Binary Translation Work on multiple instructions at once less management use sequence (of codes) without control flow change (“basic block” = BB) „dynamic BB“: ends at jump/branch instruction „static BB“ (term for compilers): ends at jump instr. or before jump target piecewise translation only when execution is required detection of instruction borders now simple (compare with static translation: what is code?) Dynamic Binary Translation Components required code cache (CC) sometimes called translation cache (TC) used to store translated basic blocks (BBs) has to be invalidated when original code is removed („unmapped“) is modified translation table maps start addresses of BBs: SPC =\u003e TPC looked up for (indirect) jumps original code and data are kept for potential accesses Code Cache properties in common with processor cashes limited size =\u003e replacement strategy (LRU, LFU, FIFO, …) processor caches have tags assoziativity cache line length coherency protocol differences to processor caches on a cache miss … differenttranslationspossibleforsameBB cachehierarchy? variable sizes of translated code interdependencies among entries (see chaining …) Code Cache: Invalidation required for self modifying code („SMC“) detection use host MMU (Memory Management Unit) guest code is write-protected on a write: invalidate all code on given page fall back: interpretation very slow with frequent modifications check for modification before every BB execution copy of original guest code required slow modification of currently running BB? Code Cache: Eviction handling Cache is full, but translation to be stored LRU evict the translated BB least recently used problems need to maintain order of uses =\u003e Overhead (time stamps / linked lists) fragmentation of cache storage – possible solution: buddy lists eviction events happen often, so need to be fast =\u003e LRU is rarely used. What is better? Goal: minimum runtime overhead Solutions: complete flush when full advantage: regular retranslation – adaptation of optimizations to current runtime behavior – old/unneeded translations get removed disadvantage – frequently used BBs have to be translated often – flush on detection of execution phase change – blocks with FIFO replacement Chaining Observation: expensive actions lookup SPC =\u003e TPC indirect jump Chaining = Linking translated BBs on known successor last guest instruction is unconditional jump conditional jump (2 cases: follow/pass-through) lookup jump targets at translation time what if successor not translated yet? on unknown successor indirect jump convert into if-then-else chains (profile targets!) return from function use shadow stack (similar to return prediction) Superblocks (SB) Motivation reduce number of jumps for given guest code (same as with chaining) larger translation units allow for more optimization possibilities (see next slides) Superblock One entry, multiple “side exits” we may have branch which go out of SB in the middle, if you have a unconditional jumps in ur original code you may just go on. I you have a conditional jump you may have a side exits at that point Combine sequence of BBs works best if the execution path of the full SB is similar to later executions (it is “hot”) =\u003e predict from past also called “trace” (“tracing JITs”) =\u003e execution path of BBs ","date":"2022-03-02","objectID":"/posts/vt/isa_emulation2/:0:1","tags":["Virtualization_Tech"],"title":"Something about ISA Emulation -- 2","uri":"/posts/vt/isa_emulation2/"},{"categories":["Notes"],"content":"Process VM Emulation of a user-level process (with possibly different ISA, different OS) – components required? – techniques? – correctness verification? Process VM with same OS, same ISA – use case? motivation process migration flexible control of used OS resources „OS-level virtualization“ enough not discussed here Virtualization of a process environment emulation of user ISA + OS environment (ABI) every guest process gets own environment guest and host OS often the same View of the host machine user processes running inside Process VM look exactly the same as host processes Wanted: easy usage of VM (automatic startup on demand) Process-level VMs provide user applications with a virtual ABI environ- ment. In their various implementations, process VMs can provide replication, emulation, and optimization. The following subsections describe each of these. Components of a Process VM Initialization – loader for code/data into guest memory – creation of tables, translation cache – redirection of exceptions ​ • for all potentially happening exceptions ​ • example: division by zero scenarios: ​ – guest does own handling ​ – guest ISA does not generate this exception Emulation Manager, translation – see ISA part emulation of OS calls – needs mapping of guest to host OS emulation of exceptions – e.g. page fault, division by zero – detection by interpreter / by hardware (OS) – reconstruction of precise state needed handling of interrupts – examples: signals, OS callbacks Types of Compatibility Strict Compatibility („Intrinsic“) – every operation of a real machine is precisely emulated by Process VM, including processor errata visible implementation behavior (e.g. „undefined“ behavior) usually not included: performance – real properties have to be known! – result: guest process is unable to detect any difference in virtual vs. real environment – complete check of VM implementation required – example: „emulation“ of Intel x86 processor by AMD HW Problems – exec has to be known completely – precise mapping can be very inefficient – high development costs Real motivation for a Process VM? – example: MS Office running on PowerPC/Linux – observation - not every operation is used - precise mapping of reality not required for execution! e.g. „don ́t care bits“ in result of a OS call – relaxation of compatibility possible Relaxed Compatibility („Extrinsic“) construction of exec\" such that exec ́ is sufficiently approximated, e.g. - only mapping for operations generated by a given compiler - implementation by similar operations (80 bit floating point emulated by 64 bit FP) „sufficiently“ defined by guest SW able to run - significantly simpler verification - ISA documentation is enough - precise mapping of operations can be replaced by more efficient, similar ones State Mapping Guest =\u003e Host Memory – direct mapping with fixed offset guest address A =\u003e host address A ́ = offset + A special case: offset = 0 – possible if guest OS allocates address ranges (i.e. user-level guest needs to be able to cope with arbitrary addresses from guest OS) – efficient emulation possible – issues available host memory smaller than guest memory 64bit guest on 32bit host =\u003e needs indirect address mapping in software (= the general resolution) Address Mapping in Software A ́ = (A \u0026 (PageLen-1)) + AddrTab[ A / PageLen] effort per access into guest memory: - temporary register + 6 host instructions + 2x host memory access - length of AddrTab: guest memory size / PageLen * pointer size host Emulation of Memory Compatibility: often a tradeoff between – performance and – compatibility Issues – how to protect VM from accesses of guest? – linear address space vs. segmentation – memory protection capabilities: Read/Write/Execute (RWE) granularity: page size – x86 can use either 4 KB or 2/4 MB – MIPS/IA-64: almost any power-of-two =\u003e Use HW if possible (e.g. mprotect + SEGF handler) Emulation of Exceptions and Interrupts Notifications from the outside that something has happen","date":"2022-02-10","objectID":"/posts/vt/process_virtual_machine/:0:1","tags":["Virtualization_Tech"],"title":"Something about Process-Virtual-Machine","uri":"/posts/vt/process_virtual_machine/"},{"categories":["Notes"],"content":"A complete ISA consists of many parts, including the register set and memory architecture, the instructions , and the trap and interrupt architecture. A virtual machine implementation is usually concerned with all aspects of ISA emulation. Here we will be focusing on (user-level) instruction emulation. Instruction set emulation can be carried out basiclly in 2 techniques: interpretation, and binary translation. Interpretation involves a cycle of fetching a source instrction, analyzing it, performing the required operation, and then fetching the next source instruction – all in software. Binary translation, on the other hand, attempts to amortize the fetch and analysis costs by translating a block of source instructions to a block of target instructions and saving the translated code for repeated use. In contrast to interpretation, binary translation has a bigger initial translation cost but a smaller execution cost. The choice of one or the otehr depends on the number of times a block of source code is expected to be executed by the guest softeware. Predictably, there are techniques that lie in between these extremes. For example, threaded interpretation eliminated the interpreter loop correstponding to the cycle mentioned earlier, and efficiency can be increased even further by predecoding the source instructions into a more efficiently interpretable intermediate form. 1 ","date":"2021-11-16","objectID":"/posts/vt/isa_emulation1/:0:0","tags":["Virtualization_Tech"],"title":"Something about ISA Emulation -- 1","uri":"/posts/vt/isa_emulation1/"},{"categories":["Notes"],"content":"Speeding up Interpretation Observation: Jumps are time consuming control conflicts in CPU pipeline lots of branch mispredictions in dispatcher What are pipeline conflicts? data/control/resource conflicts enforce pipeline stalls longer pipeline stalls longer pipeline risks longer stalls How to reduce bad effects of control conflicts? predict jump targets execute in speculative state How do branch predictors typucally work? static: on first execution (e.g. take backwards branches) dynamic: often uses table keyed by instruction address conditional: saturating counters keyed by history pattern returns: stack of recently pushed return addrresses Efficiency Guidlines for Branches “Premature optimization is the root of all evil” run performance analysis tools on finel code Reduce number of branches use inlining, also helps by specialization combine switch statments () Optimizaiton “Threaded Interpretation” Observation: Jumps are time consuming control conflicts in CPU pipeline lots of branch mispredictions in dispatcher Solution: decode next instruction at end of emulation routine of curretn instruction. Inditect vs direct This code is very similar to the indirect threaded code, except the dispatch table lookup is removed. The address of the interpreter routine is loaded from a field in the intermediate code, and a register indirect jump goes directly to the routine. Although fast, this causes the intermediate form to become dependent on the exact locations of the interpreter routines and consequently limits portability. If the interpreter code is ported to a different target machine, it must be regenerated for the target machine that executes it. However, there are programming techniques and compiler features that can mitigate this problem to some extent. Interpretation using Predecoding Motivation: Although the centralized dispatch loop has been eliminated in the indirect threaded interpreter, there remains the overhead created by the centralized dispatch table. Looking up an interpreter routine in this table still requires a memory access and a register indirect branch. It would be desirable, for even better efficiency, to eliminate the access to the centralized table. Observation: opcodes consist of multiple parts faster: one opcode (instead of op + ex_op) operands are coded in bits faster: operands aligned Properties: Space for predecoded data needed faster interpretaion significant benefit for interpretaion of CISC TPC \u0026 SPC: Why still SPC? Whenever it maybe used somewhere else You could have some kind of code which try to read the machine code itself from the PC for whatever reasons. why TPC + 1 but SPC + 4? TPC is in C, and the pre-decode array is also in C, so the compiler does the work. However, SPC + 4, if one instruction is in 4 byte. Interpretation - CISC **Potential issues with predecoding: ** much space needed better: space-tuned formats for different instructions detection of instruction borders could be data interleaved with code correct predecoding almost impossible Use a two-step process at first interpretation: do predecoding on the fly, filling predecode table all further executions: use predecoded data generated by first run [James E. Smith, Ravi Nair, Virtual Machines, 2015, ISBN:9781558609105] ↩︎ ","date":"2021-11-16","objectID":"/posts/vt/isa_emulation1/:0:1","tags":["Virtualization_Tech"],"title":"Something about ISA Emulation -- 1","uri":"/posts/vt/isa_emulation1/"},{"categories":["Notes"],"content":"Relational Model Definition relational database: a set of relations Relations : Schema: specifics name of relation, plus name and type of each column Student(sid:string, name:string, login: string, gpa: real) Instance:a table, with rows and columns Number of rows: cardinality Number of fields: degree or arity can think of as set of tuples or records Domain constrains the values that appear in a column must be drawn from the domain associated with that column relation schema: $R(f_1:D_1,…,f_n:D_n)$ $Dom_i$:set of values with domain names $D_i$ An instance of R that specifies the domain constraints in the shcema is a set of tuples with n fields: $$ Verfeinerung des relationalen Schemas “Das im Initialentwurf erzeugte relationale Schema lässt sich oftmals noch verfeinern. Dabei werden einige der Relationen eliminiert , die für die Modellierung von Beziehungstypen eingeführt worden waren. Dies ist aber nur für solche Relationen möglich, die die allgemeinen 1:1-, 1:N- oder N:1-Beziehungen repräsentieren. Die Elimination der Relationen, die die allgemeinen N:M-Beziehungstypen repräsentieren, ist nicht sinnvoll und würde i.A. zu schwerwiegenden “Anomalien“ führen. Bei der Eliminierung von Relationen gilt es folgende Regel zu beachten: Nur Relationen mit gleichen Schlüssel zusammenfassen!” 1 Vermeidung von Null-Werten ","date":"2020-11-15","objectID":"/posts/database/datenbank-3/:0:1","tags":["Database"],"title":"Relational Model and Relational Algebra","uri":"/posts/database/datenbank-3/"},{"categories":["Notes"],"content":"Relational Algebra $\\cup$ Union - Difference The difference between R and S is the set consisting of tuples that belong to R but not to S. Notated as: $$ R - S = \\lbrace t | t \\in R \\and t \\notin S \\rbrace $$ $\\delta$ Selection The selection operation is the selection of a number of tuples in a given relation R according to certain conditions F to form a new relation, denoted as: $$ \\delta_F(R) = \\lbrace t | t \\in R \\and F(t)= true \\rbrace $$ Where $\\delta$ is the selection operator, F means select event, it is a logical expression connected by the operation object (property name, constant, simple function), arithmetic comparison operator (\u003c,≤,\u003e,≥,=,≠) and logical operator ($\\land,\\lor,\\neg $), which takes the value of “true” or “false”, its basic form is: $$ $$ $\\pi$ Projection The projection operation is also a monomial operation, which selects certain attributes (columns) from a relation, rearranges these attributes, and finally removes the duplicate rows from the resulting result to obtain a new relation. That is, the vertical decomposition operation is performed on the relation from the perspective of columns, and the corresponding columns are taken out from left to right according to a specified number of attributes and order, and the duplicate tuples are deleted. $\\Join$ Join (Verbund) The join operation is a binomial operation, which is a Cartesian product of two relations in which a tuple between attributes satisfying certain conditions is selected to form a new relation, and the join is also called theta join. Der natürliche Verbund … $\\div$ Division The division operation is a binomial operation given the relations R(X, Y) and S(Y, Z), where X, Y, Z are attributes or sets of attributes. y in R and y in S can have different attribute names, but must come from the same set of domains. [Kemper, Alfons and Eickler, André, Datenbanksysteme: Eine Einführung, 7.,ISBN:9783486590180] ↩︎ ","date":"2020-11-15","objectID":"/posts/database/datenbank-3/:0:2","tags":["Database"],"title":"Relational Model and Relational Algebra","uri":"/posts/database/datenbank-3/"},{"categories":["Notes"],"content":"We know that there are different ways to represent “rotation”, and the different ways have their limitations. ","date":"2020-11-10","objectID":"/posts/imge/imge-1_en/:0:0","tags":["Intergration Methods and Devices"],"title":"Different Approaches to 「Rotation」","uri":"/posts/imge/imge-1_en/"},{"categories":["Notes"],"content":"Quaternions Quaternions are usually notated as H (for Hamiltonian reasons). From wikipedia, “Explicitly, quaternions are non-commutable extensions of complex numbers. If the set of quaternions is considered as a multidimensional real space, the quaternions represent a four-dimensional space, as opposed to a two-dimensional space of complex numbers. \" We can express it as a four-dimensional vector. $$ q = \u003cw,x,y,z\u003e q = w + xi + yi +zk q = s + v $$ – w,x,y,z are real numbers，v is a 3-dimentional vector(e.g. a 3D point), – i,j,k complex numbers (“quaternion units”) with $ i^2 = j^2=k^2=ijk=-1 $ Perhaps looking at the above equation may still be confused as to what each parameter represents, and seeing these parameters there is no specific imagination in the mind about the rotation. This is actually quite normal, if you can visualize it in your mind instead of some abnormal :) In any case, quaternions actually belong to the “remnants” of “spatial magic”. Why is that? The following is a deeper study of the quaternion by looking at it, or more accurately, the projection of the quaternion hypersphere in three. The reason why quaternions are difficult to understand is that it is a four-dimensional representation. But it is not impossible, because we usually use the unit quaternion to represent the rotation, so we only need to focus on the unit hypersphere in our thinking, and then we can more easily obtain its spherical polar plane projection (stereographic projection) in three dimensions. Projection of the unit circle in one dimension We know that the projection of the unit circle in one dimension as a two-dimensional space is an infinitely extended line. Suppose we have a plane coordinate system, with the x-axis as the real axis and the y-axis as the imaginary axis, and the intersection of the line from the point - i and any point on the circle with the x-axis constitutes its projection. It is worth noting that the line coinciding with the x-axis is only one of the projections of the unit circle in this two-dimensional space, and there is no way to represent the projections of other points in the two-dimensional space in this one-dimensional space.。 Projection of the unit ball in two dimensions Similarly, the projection of the unit sphere in two dimensions can be viewed similarly. Suppose we have the i-axis, the j-axis, two imaginary axes forming a plane, and the real axis coinciding with the z-axis. Then in a similar technique to the previous projection, we can describe the rotation of three in terms of the spherical polar plane projection. For each point on the unit sphere, we can connect it to -1, and the intersection of this line with the ij plane is the projection point in two dimensions, so that 1 on the real number axis will be projected at the origin of the plane, the point in the northern hemisphere will be projected inside the unit circle in the ij plane, and the point in the southern hemisphere will be projected in the unit circle outside the unit circle, and at infinity in any direction will be the projection of -1. The rotation is the process of “lines” gradually enclosing “circles” and “circles” gradually expanding into “lines”. Projection of unit hypersphere in 3D space Next is the focus of our attention. With the above layouts, it is easy to understand why quaternions are composed of a real term with three imaginary terms. For a spherical polar plane projection of the unit sphere in four dimensions, the 1 of the real number axis projects the origin of the ijk coordinate system. When the four-dimensional hypersphere is projected into three-dimensional space, it intersects the three-dimensional unit sphere in the same position as the three-dimensional space, and this sphere corresponds to the pure quaternion, i.e., the real part is zero. The real part between 0 and 1 is projected inside this three-dimensional sphere, while the real part less than 0 is projected outside the three-dimensional sphere, and -1 is projected ","date":"2020-11-10","objectID":"/posts/imge/imge-1_en/:0:1","tags":["Intergration Methods and Devices"],"title":"Different Approaches to 「Rotation」","uri":"/posts/imge/imge-1_en/"},{"categories":["Notes"],"content":"Explore the story of the network layer in the OSI model of computer networks.","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"Here we will recognize the two different services provided by the network layer and the core of the network layer, namely the IP protocol. We can understand: the concept of virtual interconnection networks. the relationship between IP addresses and physical addresses the traditional classification of IP addresses (including subnet masks) ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:0","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"Address Resolution Protocol ARP In practical applications, we often encounter the problem that we already know the IP address of a machine (host or router) and need to find out its corresponding hardware address. The address resolution protocol ARP is used to solve such a problem. Since it is the IP protocol that uses the ARP protocol, it is usually classified as a network layer. However, the purpose of the ARP protocol is to resolve the hardware address used at the data link layer from the IP address used at the network layer. The following is an overview of the main points of the ARP protocol. We know that the network layer uses IP addresses, but when transmitting data frames over the links of the actual network, the hardware address of that network must eventually be used. However, there is no simple mapping between IP addresses and the hardware addresses of the network below due to the different formats (for example, IP addresses have 32 bits, while the hardware addresses of a LAN are 48 bits). In addition, new hosts may often be added to a network, or some hosts may be removed. Changing network adapters can also cause the hardware addresses of hosts to change. Address Resolution Protocol ARP solves this problem by storing a mapping table from IP addresses to hardware addresses in the host’s ARP cache, and this mapping table is also dynamically updated (added or deleted on timeout) frequently. Each host has an ARP cache, which contains the mapping table from IP addresses to hardware addresses of the hosts and routers on the LAN, and these are the addresses that the host knows at present. How does the host know these addresses? We can use the following example to illustrate. When host A wants to send an IP datagram to a host B on the LAN, it first checks if there is an IP address of host B in its ARP cache. If there is, it will find out the corresponding hardware address in the ARP cache, and then write the hardware address to the MAC frame, and then send the MAC frame to this hardware address through the LAN. There may also be items where the IP address of host B cannot be checked. This may be because Host B has only just entered the network, or it may be that Host A has just been powered up and its cache is still empty. In this case, Host A runs ARP automatically and then follows the steps below to find out the hardware address of Host B. The ARP process broadcasts and sends an ARP request packet on the local area network. An example of the main content of the ARP request packet is: “My IP address is 209.0.0.5 and my hardware address is 00-00-C0-15-AD-18. I want to know the hardware address of the host with IP address 209.0.0.6.” This ARP request packet is received by all ARP processes running on all hosts on this LAN. The IP address of host B matches the IP address to be queried in the ARP request packet, so it receives this ARP request packet and sends an ARP response packet to host A, and writes its own hardware address in this ARP response packet. Since the IP addresses of all the other hosts do not match the IP address in the ARP request packet, they ignore the ARP request packet. The main content of the ARP response packet is: “My IP address is 209.0.0.6, and my hardware address is 08-00-2B-00-EE-0A.” Note that although the ARP request packet is sent broadcast, the ARP response packet is plain unicast, i.e., it is sent from a source address to a destination address. Once host A receives the ARP response packet from host B, it writes the mapping from host B’s IP address to the hardware address in its ARP cache. When host A sends a datagram to B, it is likely that host B will have to send another datagram to A shortly thereafter, and thus host B may also have to send an ARP request packet to A. To reduce the amount of traffic on the network, host A writes its IP address to the hardware address mapping into the ARP request packet when it sends its ARP request packet. When host B receives A’s ARP request packet, it writ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:1","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"IPv4 The format of an IP datagram can illustrate what functions the IP protocol has. An IP datagram consists of two parts: the header and the data. The first part of the header is a fixed length of 20 bytes, which is mandatory for all IP datagrams. Following the fixed part of the header are optional fields that are variable in length. The following describes the meaning of each field in the header. Version Occupies 4 bits and refers to the version of the IP protocol. The version of the IP protocol used by both communicating parties must be the same. The current widely used IP protocol version number is 4 (i.e. IPv4). Header Length The maximum decimal value that can be represented is 15. Please note that the unit of the number represented by the initial length field is a 32-bit word (one 32-bit word is 4 bytes long). Since the fixed length of the IP prefix is 20 bytes, the minimum value of the prefix length field is 5 (i.e., the binary representation of the prefix length is 0101). And when the prefix length is the maximum value 1111 (i.e., 15 in decimal), it indicates that the prefix length reaches a maximum of 15 32-bit words long, i.e., 60 bytes. When the IP packet’s initial length is not an integer multiple of 4 bytes, it must be padded using the final padding field. Therefore, the data portion of an IP datagram always starts at an integer multiple of 4 bytes, which is more convenient when implementing IP protocols. The disadvantage of limiting the initial length to 60 bytes is that sometimes it may not be enough. However, this is done in the hope that the user will minimize the overhead. The most common prefix length is 20 bytes (i.e., a prefix length of 0101), at which point no options are used. Differentiated Services It takes up 8 bits and is used to get better service. This field was called Service Type in the old standard , but it was never actually used. Total Length The total length is the length of the sum of the header and data in bytes. The total length field is 16 bits, so the maximum length of a datagram is 2 16 - 1 = 65535 bytes. However, in practice, it is very rare to transmit such a long datagram. ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:2","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"Flow of IP layer forwarding packets We know that there is a routing table in the router, but if the routing table points to each host how to forward, that would make the routing table too large, but if the routing table only points to a network how to forward, then the routing table will only include the number of network items, which greatly reduces the need to store items, we do not have to care about the specific topology of a network and the specific number of devices connected to the network, because from one router to the next router forwarding. The most important items in the routing table are the following two pieces of information: $$ (Destination network address, next-hop address) $$ ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:3","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"ICMP(Internet Control Message Protocol) In order to forward IP datagrams more efficiently and to improve the chances of successful delivery, the ICMP protocol is used at the network layer. The ICMP protocol allows hosts or routers to report errors and provide reports on exceptions. It is important to note that ICMP is not a high-level protocol, but rather remains a network layer protocol, as ICMP messages are packed into IP datagrams as part of the data. “TOS” and “Protocol” are related to ICMP in the introduction to the packet header structure. The description given in the professor’s slide is also great: Das Internet Control Message Protocol (ICMP) dient dazu, • in derartigen Fällen den Absender über das Problem zu benachrichtigen und • stellt zusätzlich Möglichkeiten bereit, um z. B. • die Erreichbarkeit von Hosts zu prüfen („Ping“) oder • Pakete umzuleiten (Redirect). Types of ICMP messages ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:4","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"IPv6 IPv6 Basic Header After IPv4 ran out of addresses, we naturally needed to expand the IP address block, hence IPv6. IPv6 still supports connectionless transport, but refers to the protocol data unit PDUs as packets, rather than IPv4 datagrams. The main changes in IPv6 are as follows: Larger address space, increasing the 32-bit IPv4 address space to four times that of IPv4, i.e., to 128 bits. Extended address hierarchy. Flexible prefix format. Improved options. 5. Allows protocols to continue to expand. 6. IPv6 prefixes are now 8-byte aligned (i.e., the prefix length must be an integer multiple of 8 bytes), whereas IPv4 prefixes were 4-byte aligned. Compared to IPv4, IPv6 has changed some fields in the header as follows: The length field of the header has been removed, since the length of the header is fixed (40 bytes) The Type of Service (TOS) field has been removed, since the Priority and Flow Label Number fields implement the function of the Type of Service field The total field length has been removed in favor of the payload length field Eliminated the Identification, Flag and Slice Offset fields, but the role is the same The protocol field has been removed Check sum field has been removed Removed optional fields IPv4-Header (oben) und IPv6-Header (unten) im Vergleich IPv6 Basic Prefix Segments Role IPv6 Header version 4 bits, specifies the protocol version, for IPv6 this field is 6 traffic class 8 bits. This is to distinguish the class or priority of different IPv6 datagrams. We are currently experimenting with different traffic performance. flow label 20 bits. A new mechanism in IPv6 is to support resource preallocation and allow routers to associate each datagram with a given resource allocation. IPv6 introduces the abstract concept of flow. A “flow” is a series of datagrams () on an interconnected network from a specific source to a specific destination (unicast or multicast), and the routers in the path of this “flow” are guaranteed the specified quality of service. All datagrams belonging to the same stream have the same stream label. Therefore, stream tagging is particularly useful for real-time audio/video data delivery. 4. payload length is 16 bits. It specifies the number of bytes in the IPv6 datagram in addition to the base header (all extended headers are counted as part of the payload). The maximum value of this field is 64KB (65535 bytes) next header 8 bits. It is equivalent to the optional field of the IPv4 protocol field. hop limit 8 bits. Used to prevent datagrams. Source address 128 bits. It is the address of the sender side of the datagram. 8. Destination address 128 bits. It is the IP address of the receiving end of the datagram. Let’s introduce the extended initials of IPv6. As you know, if an IPv4 datagram uses options in its header, each router along the path of the datagram must check each of these options, which slows down the processing of the datagram by the router. IPv6 puts the functionality of the options in the original IPv4 header in the extended header, and leaves the extended header to the hosts at the source and destination at each end of the path, while none of the routers** through which the datagram passes process the extended header** (with the exception of one header). (with the exception of one, the hop-by-hop option for extended headers), which ** greatly improves the processing efficiency of the router **. We will probably encounter these six extended headers: (1) hop-by-hop option; (2) routing; (3) fragmentation; (4) identification; (5) encapsulation security payload; and (6) destination option. Each extension header consists of a number of fields, and they vary in length. However, the first field of all extension headers is the 8-bit “next header” field. The value of this field indicates what field follows the extension header. When multiple extension headers are used, they should appear in the above order. High-level headers are always placed last. IPv6 Addresses n general, ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:5","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"Routing Longest Prefix Matching When using CIDR, since this notation of network prefix is used, the IP address consists of two parts: network prefix and host number, so the items in the routing table have to be changed accordingly. At this time, each item consists of “network prefix” and “next-hop address”. But when looking up the routing table you may get more than one match . This raises the question: which route should we choose from these matches? The correct answer is: the route with the longest network prefix should be selected from the match results. This is called longest-prefix matching, because the longer the network prefix, the smaller the address block, and therefore the more specific the route. Longest-prefix matching is also known as longest matching or best matching. Die Routingtabelle wird von längeren Präfixen (spezifischeren Routen) hin zu kürzeren Präfixen (weniger spezifische Routen) durchsucht. Der erste passende Eintrag liefert das Gateway (Next-Hop) eines Pakets. Diesen Prozess bezeichnet man als Longest Prefix Matching. ","date":"2020-07-06","objectID":"/posts/computernetwork/network-layer_en/:0:6","tags":["computer network"],"title":"The network layer as a 'lead' expert","uri":"/posts/computernetwork/network-layer_en/"},{"categories":["Notes"],"content":"first draft","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - Boosting","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"This time, I brought the first draft of the paper to be submitted for the seminar, which is a way to be lazy. ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:0:0","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - Boosting","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"A brief Overview on Shared Memory Parallelism in Parallel Computing ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:0","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - Boosting","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"1. Why parallel? To speed up, while we are facing the limitation of current transistors and increasing energy consumption. Now that we know it is necessary and lots of privilege besides you need to reconstruct your program yourself rather than automatically distributed by APIs in a serial way. So, it then leads to the following question: how do we write parallel programs? Truth be told, there are number of possible answers to this question, while most of them share the idea of partitioning the work among cores. The two commonly used approach for this: task-parallelism and data-parallelism. In task-parallelism, we partition the problems into separately tasks that will be carried out in cores. While in data-parallelism each core carries out roughly similar operations on its part of data. ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:1","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - Boosting","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"2. As it was mentioned above, when we write programs that are explicitly parallel, we will be focusing on two major types of parallel systems: shared-memory and distributed-memory. What is the idea of shared memory system? In a shared memory system, processors are connected via an interconnection network, so that every core can access ach memory location. And there are different hardware structures in implementing this idea. In shared-memory system all processors are either connected directly to the main memory or have their own memory blocks and are accessible to each other through special hardware build into the processors. Anyway, shared memory parallel computers may have different implementations, but generally have in common the ability for all processors to access all memory as global address space, which means multiple processors can operate independently but share the same memory resources. Furthermore, changes in a memory location operated by one processor are visible to all other processors. Interconnection topologies Thus, the first type of system is called a uniform memory access, or UMA, system, while the second type is called a nonuniform memory access, or NUMA, system. In UMA systems the time for every core to access the memory locations is the same, while in NUMA access time from one cache to distributed data parts varies as topology place. The NUMA Systems In a Nonuniform Memory Access architecture, each CPU has a memory module physically next to it, Advantages: Global address space provides a user-friendly programming perspective to memory Data sharing between tasks is both fast and uniform due to the proximity of memory to CPUs Disadvantages: Primary disadvantage is the lack of scalability between memory and CPUs. Programmer responsibility for synchronization constructs that ensure “correct” access of global memory. Interconnection networks This is important in implementing hardware of shared-memory parallelism: the efficiency of data exchange between memory and processors have huge impact on final execution time. Shared-memory interconnects Here we will explain two most widely used interconnects in shared-memory systems: buses and crossbars. The key property for a bus is that the devices connected to it share the communication wires. Since the amount of our cores is not many, it is more flexible to use a bus. However, the expected performance decreases as the number of cores connected to the bus increases. Thus, buses are replaced by switched interconnects in a more complicated shared-memory system. As name suggests, switched interconnects use switches to control the data among the connected devices. (Figure of crossbars) The individual switches can be shown as one from following figure (Figure of crossbar status) Crossbar switches are too expensive for large -scale systems, but are useful in some small systems. However, we can simplify the idea, that leads us to Omega (or Delta) Interconnects, which is similar to crossbars, but with fewer paths. (Figure of Omega Interconnects) Cache Issues Before we go further on this topic, let us firstly recall the idea of caching. To solve the problem of redundant time of processors accessing data in main memory we added block of Cache Coherency Explanation: To understand these issues, suppose we have a shared-memory system with two cores, each of which has its own private data cache. As long as the two cores only read share data, there is no problem. For example, suppose that x is a shared variable that has been initialized to value 3. In one cache we have the instruction that changes the value of x to 7, while in another cache at the same time also have operation concerning the value of x, which leads to the problem of 3 or 7? (could be a figure involving things describes above) However, this will be unpredictable situation when situations mentioned above occur regardless of whether the system using which policy among processors, because this occurs in caches with in ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:2","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - Boosting","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"3. OpenMP is an API designed for programming shared-memory parallel programming. The MP in OpenMP stands for “multiprocessing”. Something good about OpenMP is that the programmer does not need to specify how each thread behave explicitly, which suggests that OpenMP allows the programmer to simply mark that the block of code should be executed in parallel, and the exact determination of which thread should execute them is handed to the compiler and the run-time system. In other word, OpenMP requires more compiler support rather works like a library of functions. This convenience in writing parallel program does not come at no cost: we give away the power to program virtually any possible thread behaviour in exchange. (give an example of a program in OpenMP, then introduce the basic rules of writing program with OpenMP) ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:3","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - Boosting","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"4.Usage in high performance computing Parallelism not always make the execution faster, sometimes the more parallelism we had, the slower the program ran, which means we need to reconsider the task before we choose to implement it parallelly. ​ (table with run time of Dijkstra with 1000 nodes but different number of threads) ​ (table with run time of Dijkstra with 25000 nodes but different number of threads) Possible way to improve our performance when writing parallel program with OpenMP on shared-memory systems: False sharing could be a problem. Example to this: analysis the execution of a Matrix multiplies a vector. $Y = A * x$ #pragma omp parallel for num_threads(thread_count) \\ default(none) private (i, j) shared(A, x, y, m, n) for(i =0; i\u003c m; i++){ y[i] = 0.0; for(j = 0; j \u003c n; j++){ y[i] += A[i][j] * x[j]; } } We will compare the performance of the matrix 8000000 x8 or 8000000 x 8 or 8 x 8000000 (efficiency table here) Although we may face much larger numbers in high performance computing Why is multi threads worse than less threads? Suppose for the moment that threads 0 and 1 are assigned to one of the processors and threads 2 and 3 are assigned to the other. Also suppose that for the 8x8,000,000 problem all of y is stored in a single cache line. Then every write to some element of y will invalidate the line in the other processor’s cache. For example, each time thread 0 updates y[0] in the statement. $y[i] += A[i][j] * x[j];$ if thread 2 or 3 is executing this code, it will have to reload y. Each thread will update each of its components 8,000,000 times. We see that with this assignment of threads to processors and components of y to cache lines, all the threads will have to reload y many times. This is going to happen in spite of the fact that only one thread accesses any one component of y, for example, only thread0 accesses $y[0]$. (Explain why false sharing may not be a problem here) Possible ways of avoiding false sharing in the matrix multiplication program: One possible solution is to “pad” y vector with dummy elements in order to guarantee that any update by one thread won’t influence another cache line. Another alternative way is to have its own private storage during the loop, and then update the shared storage when iterations are done. ","date":"2020-06-07","objectID":"/posts/seminar-hpc/shared-memory-parallelism_2/:1:4","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - Boosting","uri":"/posts/seminar-hpc/shared-memory-parallelism_2/"},{"categories":["Notes"],"content":"a small algorithms for practical course for real-time computer graphics","date":"2020-05-21","objectID":"/posts/computer_graphics/implementation-about-diamond-square/","tags":["real-time computer graphics"],"title":"Generation of height maps based on diamond-square-algorithm in C++","uri":"/posts/computer_graphics/implementation-about-diamond-square/"},{"categories":["Notes"],"content":"其实很多时候我们看到的这些渲染真的是很神奇的事，比如一些terrain看似是三维的，其实可以由二维产生，即为每一个像素点赋予一个高度值，这些高度值构成的点列又可以转换为一张二维图像来存储，需要的时候，相应的地形可以由此图片生成。淡淡的想，一些所谓的空间法术，大概也就是这样了吧。高度图作为维度空间法术的入门想来也是极好的。 至于法术的入门，我们只关注用正态分布或diamond square来赋值，terrain generator中关于color和normal的值图像我们后面会聊到。 下面来看代码： #include \u003ciostream\u003e#include \u003ctchar.h\u003e#include \u003cwinnt.rh\u003e #include \u003csstream\u003e#include \u003crandom\u003e #include \u003cSimpleImage.h\u003e#include \u003cTextureGenerator.h\u003e#include \u003ctime.h\u003e #include \u003cvector\u003e //#pragma comment(lib, \"GEDUtilsd.lib\") //Declare functions float squareStep(float* field, int x, int y, int reach, int width); float diamondStep(float* field, int x, int y, int reach, int width); float random(float min, float max); //GEDUtils::SimpleImage transfer(float* field, int width); void smoothArray(float* field, int64_t width, int64_t height); void diamondSquare(float* field, int size, int width, float roughness); using namespace std; // Define a macro for easier access to a flattened 2D array #define IDX(x, y, w) ((x) + (y) * (w)) //Define the size of an array #define ARR_LEN(array, length){length = sizeof(array)/sizeof(array[0]);} /* int main() { std::cout \u003c\u003c \"Hello World!\\n\"; } */ //use for debugging void printArray(float* field, uint64_t width, uint64_t height) { for (uint64_t y = 0; y \u003c height; y++) { for (uint64_t x = 0; x \u003c width; x++) std::cout \u003c\u003c field[IDX(x, y, width)] \u003c\u003c \" \"; std::cout \u003c\u003c std::endl; } } int _tmain(int argc, _TCHAR* argv[]) { //int len; /* ARR_LEN(argv, len); if (argc != len) { throw \"invalid length\"; } */ cout \u003c\u003c endl \u003c\u003c \"argc = \" \u003c\u003c argc \u003c\u003c endl; cout \u003c\u003c \"Command line arguments received are:\" \u003c\u003c endl; for (int i = 0; i \u003c argc; i++) cout \u003c\u003c \"argument \" \u003c\u003c (i + 1) \u003c\u003c \": \" \u003c\u003c argv[i] \u003c\u003c endl; if (_tcscmp(argv[1], TEXT(\"-r\")) != 0 || _tcscmp(argv[3], TEXT(\"-o_height\")) != 0 || _tcscmp(argv[5], TEXT(\"-o_color\")) != 0 || _tcscmp(argv[7], TEXT(\"-o_normal\")) != 0) { throw \"do not match '-r' or '-o's\"; } //alternative way for resolution //std::wstringstream wsstream; //wsstream \u003c\u003c argv[2]; //int resolution; //wsstream \u003e\u003e resolution; int resolution = _tstoi(argv[2]); if (resolution \u003c= 0) { throw \"\u003c= 0 for resolution\"; } //preparation for normal mapping std::default_random_engine e; std::normal_distribution\u003cfloat\u003e n(0, 1); int length = resolution * resolution; float* field = new float[length]; //mapping normal distribution for (int y = 0; y \u003c resolution; y++) { for (int x = 0; x \u003c resolution; x++) { float value = n(e); while (value \u003c 0.0f || value \u003e 1.0f) { value = n(e); } field[IDX(x, y, resolution)] = value \u003e= 0 ? value : -value; } } //name of height file wstring wsh(argv[4]); const wchar_t* cstrh = wsh.c_str(); //name of color file wstring wsc(argv[6]); const wchar_t* cstrc = wsc.c_str(); //name of normal file wstring wsn(argv[8]); const wchar_t* cstrn = wsn.c_str(); //generate a texture generator GEDUtils::TextureGenerator texGen(L\"..\\\\..\\\\..\\\\..\\\\external\\\\textures\\\\gras15.jpg\", L\"..\\\\..\\\\..\\\\..\\\\external\\\\textures\\\\ground02.jpg\", L\"..\\\\..\\\\..\\\\..\\\\external/textures/kork02.jpg\", L\"..\\\\..\\\\..\\\\..\\\\external\\\\textures\\\\rock1.jpg\"); try { //this is for normal distribution /* //copy the created normal distribution values to the heightfield GEDUtils::SimpleImage heightImage = GEDUtils::SimpleImage::SimpleImage((UINT)resolution, (UINT)resolution); for (int y = 0; y \u003c resolution; y++) { for (int x = 0; x \u003c resolution; x++) { heightImage.setPixel(x, y, field[IDX(x, y, resolution)]); } } //save the generated heightField if (!heightImage.save(cstrh)) { throw \"Could not save heightField image\"; } // Load height image into the image test //GEDUtils::SimpleImage test(cstrh); //create a vector to store vector\u003cfloat\u003e vectorHeight(field, field + length); //this is for normal distribution //texGen.generateAndStoreImages(vectorHeight,resolution - 1,cstrc,cstrn); delete[] field; */ //this is for diamond square int length2 = (resolution + 1) * (resolution + 1); float* field2 = new float[length2]; //assign random value to the corners field2[IDX(0, 0, resolution + 1)] = random(0, 1); field2[I","date":"2020-05-21","objectID":"/posts/computer_graphics/implementation-about-diamond-square/:0:0","tags":["real-time computer graphics"],"title":"Generation of height maps based on diamond-square-algorithm in C++","uri":"/posts/computer_graphics/implementation-about-diamond-square/"},{"categories":["Notes"],"content":"有穷自动机笔记2","date":"2020-05-14","objectID":"/posts/theo/something-about-dfa-and-nfa-2/","tags":["theory of computation","theoretical computer science"],"title":"DFA and NFA -- 2","uri":"/posts/theo/something-about-dfa-and-nfa-2/"},{"categories":["Notes"],"content":"From part 1 we can recognize what DFA and NFA look like and know the equivalence between them. In “From Infinite Automata to Regular Languages” we know the equivalence between DFA, NFA and regular expressions and regular languages, and there are some tools such as pump priming and Ardens Lemma to either prove or convert them. We have made some progress in the ‘art’ level so far, so we might as well go deeper in the ‘road’ or ‘art’ level. Why do you want to think about poor automata and regular languages? What can they be used for? This is an “Entscheidungsverfahren”, i.e. a “judgement process”, how to determine if an existing infinite automaton or regular expression has a certain property X? This is perhaps too abstract, but let’s use a concrete problem: if there is a D, and D is a DFA or NFA or RE or rechtlineare Grammatik … , to determine the following problems: Wortproblem: to determine whether a word is recognized by D; Leerheitproblem: whether the language accepted by D is the empty set; Endlichkeitsproblem: whether the language produced by D is finite or infinite; Äquivalenzproblem: whether for D1 and D2 they are are equivalent. So how to determine that these problems are judiciable? This goes to the more central aspect of theoretical computing: if there is an algorithm that can give the correct output in finite time, then it is judiciable. Now it is good to see the specific lemma of the above four issues： Lemma 3.36 Das Wortproblem ist für ein Wort w und DFA M in Zeit O(|w| + |M|) entscheidbar. Lemma 3.37 Das Wortproblem ist für ein Wort w und NFA N in Zeit O($|Q|^2$|w| + |N|) entscheidbar. Beweis： Sei Q = {1, … ,s}, $q_0 = 1 und w = a_1 … a_n$. S:= {1} for i := 1 to n do S:= $\\cup _{j \\in S} \\delta (j, a_i)$ return ($S \\cap F \\neq \\emptyset$) Lemma 3.38 Das Leerheitsproblem ist für DFAs und NFAs entscheidbar (in Zeit O(|Q||$\\Sigma$|) bzw. O($|Q|^2 |\\Sigma|$)). Beweis: L(M) = $\\emptyset$ gdw kein Endzustand von $q_0$ erreichbar ist. Dies ist eine einfache Suche in einem Graphen, die jede Kante maximal ein Mal benutzen muss. Ein NFA hat $\\leq |Q|^2|\\Sigma|$ Kanten, ein DFA hat $\\leq |Q||\\Sigma|$ Kanten. Ist $\\Sigma$ fix, z.B. ASCII, so wird daraus O($|Q|^2$) bzw O(|Q|). Lemma 3.39 ","date":"2020-05-14","objectID":"/posts/theo/something-about-dfa-and-nfa-2/:0:0","tags":["theory of computation","theoretical computer science"],"title":"DFA and NFA -- 2","uri":"/posts/theo/something-about-dfa-and-nfa-2/"},{"categories":["Notes"],"content":"数据链路层介绍","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer_en/","tags":["computer network"],"title":"Functionally important and single data link layer","uri":"/posts/computernetwork/data-link-layer_en/"},{"categories":["Notes"],"content":"Verbindungscharakterisierung The connection between two nodes will have various properties as follows：Übertragungsrate, Übertragungsverzögerung, Übertragungsrichtung, and Mehrfachzugriff (Multiplexing)。 ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer_en/:1:0","tags":["computer network"],"title":"Functionally important and single data link layer","uri":"/posts/computernetwork/data-link-layer_en/"},{"categories":["Notes"],"content":"Point-to-point channels Übertragungsrate The time that the data is placed on top of the channel we call Serialisierungszeit and is denoted as $t_s$, L is the size of the data to be transferred, r is the transfer rate, and so $$ t_s = \\frac{L}{r} $$ Here, $t_s$ can be thought of as the transmission delay, the time that will be used to transmit this data. Ausbreitungsverzögerung As the name implies, the propagation delay, unlike the above, is the time it will take for the signal to travel from one segment to the other in the channel. $$ t_p = \\frac{d}{vc_0} $$ $c_0$is the speed of light, and v is a constant, related to the transmission medium. Gesamtverzögerung $t_d$ That is, the total delay is the sum of the above two items Bandbreitenverzögerungsprodukt This is known as broadband delay, precisely because propagation in the channel inevitably takes time and we need a certain capacity to store. $$ C = t_p r = \\frac{d}{vc_0} r $$ The unit is bit. Übertragungsrichtung Übertragungsrichtung ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer_en/:1:1","tags":["computer network"],"title":"Functionally important and single data link layer","uri":"/posts/computernetwork/data-link-layer_en/"},{"categories":["Notes"],"content":"Data Link Layer for Broadcast Channels Generally speaking here we will use time division multiplexing (Zeitmultiplex), i.e. the use of a channel to achieve communication between different ports, based on a non-deterministic approach (concurrent access) in packet networks (Ethernet, wireless LANs). As for multiplexing I think the diagram above the professor’s SLIDE is clear: Multiplexing ALOHA and Sorted ALOHA CSMA, CSMA/CD, CSMA/CA CSMA actually refers to Carrier Sense Multiple Access, which is a simple optimization of the previous sorted ALOHA, i.e. “listen before talk”.。 CSMA/CD Known as Carrier Listening Collision Listening Multipoint Access/Collision Detection, which is the protocol used by Ethernet, divided into two parts: i.e. “Listening before sending”, each station has to detect whether there are other stations on the bus sending data before sending data, and if so, temporarily do not send data and wait for the channel to be free before sending. There is no “carrier” on the bus, it is just a customary name here; - Collision detection: i.e. “listen while sending”, the adapter detects the signal voltage on the channel while sending data, so as to determine whether other stations are also sending data when it is sending data. The adapter detects the signal voltage on the channel while sending data in order to determine whether other stations are sending data while it is sending data. When the signal voltage on the bus varies greatly when the data is sent at the same time and exceeds a certain threshold value, it is considered that at least two stations on the bus are sending data at the same time, indicating a collision. In this case, the signal on the bus is distorted and cannot be recovered. Therefore, every station that is sending data, once a collision is detected, the adapter has to stop sending immediately to avoid wasting network resources and wait for a random period of time before sending again. If the propagation delay of the upper signal over the link is considered, the process is similar to this. CSMA/CD Each endpoint has the possibility to encounter a collision within a short period of time after it sends the data, which is the maximum time for two one-way trips, and this time is called the “contention period”. Only by passing the contention period can we be sure that the transmission will not collide. It is for this reason that Ethernet specifies a minimum length of 64 bytes for data frames, and all frames less than this length are considered to be dropped due to collisions. When we receive at least 64 bytes we can assume that there is no collision between them. CSMA/CD cannot transmit and receive at the same time, so it is Halbdulplex, which is a half-duplex protocol, i.e. alternating communication in both directions. CSMA/CA CSMA/CD cannot be used in wired LANs (wireless LANs), because even if the sent message is long enough, collision cannot always be detected. CA here is actually colision avoidance, which means that it cannot detect conflicts on the channel while sending packets, but can only try to “avoid” them. For example, if computer A and computer C send a control message to computer B at the same time, they will arrive at computer B at the same time, resulting in a conflict. When such a conflict occurs, the sender can wait for a random period of time, and then resend the control message. Because control messages are much shorter than data frames, the likelihood of a second conflict is also much lower than with traditional Ethernet. Eventually a control message will arrive correctly, and then Computer B sends a response message. Usually CSMA/CA uses the ACK signal to avoid conflicts, that is, the client only confirms that the sent data has arrived correctly at the destination when it receives the ACK signal back on the network. Here’s what the professor’s slide says： Wenn Medium frei, übertrage mit Wahrscheinlichkeit p oder verzögere mit Wahrscheinlichkeit 1 − p um eine feste Zeit dann 1. Wenn Mediu","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer_en/:1:2","tags":["computer network"],"title":"Functionally important and single data link layer","uri":"/posts/computernetwork/data-link-layer_en/"},{"categories":["Notes"],"content":"Encapsulation into frames Encapsulation simply means splitting and packing packets in advance, attaching the destination address, local address, and some bytes for error correction to the packet being sent. The rules that are followed and negotiated between the two communicating parties when processing the packet are the protocol. The packet transmitted at the network layer is transmitted in the data link layer as a frame (Rahmen). The packet arrives at the data link layer, and the protocol header and protocol tail of the data link layer are added to form a data frame. ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer_en/:2:0","tags":["computer network"],"title":"Functionally important and single data link layer","uri":"/posts/computernetwork/data-link-layer_en/"},{"categories":["Notes"],"content":"MAC Address is the address in the ROM of the adapter (hence the name adapter address or adapter identifier EUI-48) that is solidified in each computer on the LAN, determined by the hardware vendor, and does not change. As long as the adapter remains the same, it remains the same. It is the “name” or identifier of each station. If a host or router connected to the LAN has more than one adapter installed, then such host or router has more than one “address”, that is, this 48-bit address should be the identifier of an interface. Of course, it is possible to change the mac address with the appropriate software. MAC Frame Template ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer_en/:2:1","tags":["computer network"],"title":"Functionally important and single data link layer","uri":"/posts/computernetwork/data-link-layer_en/"},{"categories":["Notes"],"content":"Error Detection ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer_en/:3:0","tags":["computer network"],"title":"Functionally important and single data link layer","uri":"/posts/computernetwork/data-link-layer_en/"},{"categories":["Notes"],"content":"CRC residual test ","date":"2020-05-12","objectID":"/posts/computernetwork/data-link-layer_en/:3:1","tags":["computer network"],"title":"Functionally important and single data link layer","uri":"/posts/computernetwork/data-link-layer_en/"},{"categories":["Notes"],"content":"有穷自动机中的部分java实现","date":"2020-05-12","objectID":"/posts/theo/something-about-regular-expression-implementation/","tags":["theory of computation","theoretical computer science"],"title":"From Finite Automata to Regular Languages - Implementation Part","uri":"/posts/theo/something-about-regular-expression-implementation/"},{"categories":["Notes"],"content":"This article can be seen as “from infinite automata to regular expressions” of this additional article, mainly to provide JAVA to achieve from DFA to RE conversion. Because the code is copied from the assignment, I don’t know if there will be any copyright problem, but this is not considered commercial use, and some of the code is written by me, so it should not be a big problem. (The assignment will give some source code of Java and Haskell by default, I don’t want to write Haskell so I will use Java to implement it, interested parties can try it by themselves after copying the code) Let’s first look at the implementation of regular expressions, where there is an abstract class of Regex, corresponding to the definition of regular expressions in another blog post, with six subclasses to serve as a basis for generalization, namely $\\epsilon , \\emptyset$, single-letter, concatenated, and, asterisk, and here is the code. import java.util.Arrays; import java.util.Collections; import java.util.HashMap; import java.util.HashSet; import java.util.Iterator; import java.util.LinkedList; import java.util.List; import java.util.Map; import java.util.Set; import java.util.stream.Stream; public abstract class Regex { /** * Determines whether the given string matches the regular expression. * @param word the string * @return */ public abstract boolean matches(String word); private static final class ReadOnlyIterator\u003cT\u003e implements Iterator\u003cT\u003e { private final Iterator\u003cT\u003e it; public ReadOnlyIterator(Iterator\u003cT\u003e it) { this.it = it; } @Override public boolean hasNext() { return it.hasNext(); } @Override public T next() { return it.next(); } } public static final class Empty extends Regex { @Override public boolean matches(String word) { return false; } @Override protected void appendTo(StringBuilder sb, int prec) { sb.append(\"{}\"); } private Empty() { } @Override protected int getPrecedence() { return 4; } @Override public int hashCode() { return 23; } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; return (getClass() == obj.getClass()); } public int size() { return 1; } @Override public boolean isNullable() { return false; } @Override public \u003cR\u003e R accept(Visitor\u003cR\u003e vis) { return vis.visitEmpty(); } } public static final class Epsilon extends Regex { @Override public boolean matches(String word) { return word.isEmpty(); } private Epsilon() { } @Override protected void appendTo(StringBuilder sb, int prec) { sb.append(\"()\"); } @Override protected int getPrecedence() { return 4; } @Override public int hashCode() { return 47; } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; return (getClass() == obj.getClass()); } public int size() { return 1; } @Override public boolean isNullable() { return true; } @Override public \u003cR\u003e R accept(Visitor\u003cR\u003e vis) { return vis.visitEpsilon(); } } public static final class Single extends Regex { private final char c; @Override public boolean matches(String word) { return word.length() == 1 \u0026\u0026 word.charAt(0) == c; } @Override protected int getPrecedence() { return 4; } public char getChar() { return c; } private Single(char c) { super(); this.c = c; } @Override protected void appendTo(StringBuilder sb, int prec) { sb.append(c); } @Override public int hashCode() { final int prime = 31; int result = 1; result = prime * result + c; return result; } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Single other = (Single) obj; if (c != other.c) return false; return true; } public int size() { return 1; } @Override public boolean isNullable() { return false; } @Override public \u003cR\u003e R accept(Visitor\u003cR\u003e vis) { return vis.visitSingle(c); } } public static final class Concat extends Regex implements Iterable\u003cRegex\u003e { private final List\u003cRegex\u003e children; private final int size; private final boolean nullable; pri","date":"2020-05-12","objectID":"/posts/theo/something-about-regular-expression-implementation/:0:0","tags":["theory of computation","theoretical computer science"],"title":"From Finite Automata to Regular Languages - Implementation Part","uri":"/posts/theo/something-about-regular-expression-implementation/"},{"categories":["Notes"],"content":"初识高性能计算","date":"2020-05-03","objectID":"/posts/seminar-hpc/shared-memory-parallelism_1/","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - a precursor","uri":"/posts/seminar-hpc/shared-memory-parallelism_1/"},{"categories":["Notes"],"content":"I’m going to start a new series on the Theo and Network series that I’m currently working on, which is the parallel computing series that I’ve seen so far. This semester, I was assigned the topic “Shared Memory Parallelization”, which is the title of the seminar. Because I need to submit a paper and do a presentation at the end, I think I can also open a new series to organize and record the new knowledge I learned when I read various materials, of course, because the materials I read are in English, so in order to write a paper afterwards, I sometimes write directly in English. The main content of this article is the recent outline to be submitted and some introduction about parallel computing. General concept why parallel? what is the idea of shared memory? memory modules Hardware views Different architecture UMA Systems Pro Nach NUMA Systems Pro Nach How do interconnection networks work? basic ideas that distiguish from normal networks switch, crossbar…. Cache issues Cache coherence: Snooping cache coherence Directory-based cache coherence False sharing: Basic explanaton, will have further discussion in programmer’s view Programmer’s view parallel programming models shard memory without threads threads model: some basic concept and implementation with OpenMP or CUDA some example parallel programs written in OpenMP that may concerning to solving flase sharing Difference in HPC Distiguish what’s special about shared memory in HPC (still not quite clear, need further readings) Materials related: Prgramming on Parallel Machines, Norm Matloff, Univercity of California, Davis An Introduction to Parallel Programming, Peter Pacheco Parallel Computer Architecture A Hardware / Software Approach, David Culler, Jaswinder Paul Singh Parallel Programming for Modern High Performance Computing Systems, Paul Czarnul Slide of an old lecture from TUM: https://www5.in.tum.de/lehre/vorlesungen/parhpp/SS08/ ","date":"2020-05-03","objectID":"/posts/seminar-hpc/shared-memory-parallelism_1/:0:0","tags":["parallel computing","Seminar-HPC"],"title":"What we're talking about when we talk about shared memory parallelism - a precursor","uri":"/posts/seminar-hpc/shared-memory-parallelism_1/"},{"categories":["Notes"],"content":"正则语言笔记","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"在上篇中我们了解到DFA与NFA的故事，从中我们初识了正则运算并且了解到其封闭性(虽然我还没有写完)，在本篇中我们将会里了解到正则表达式，进而到正则语言，我们可以从这个小小的角度对归纳的魅力有惊鸿一瞥，还可以了解到有穷自动机，正则表达式，正则语言它们之间的转换。 ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:0","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"正则表达式及形式化定义 正则表达式其实是正则语言(formal language)的另一种定义。 我们不妨来看看定义，这里其实就可以就感受到归纳的威力了！ R是正则表达式(RE)当且仅当R是 a, $a \\in \\Sigma$; /表示 {a}/ $\\epsilon$ /表示 ${\\epsilon}$/ $\\emptyset$ $(R_1 \\cup R_2)，R_1和 R_2都是正则表达式；$ $(R_1 R_2), R_1和R_2都是正则表达式；$ $(R_1^ *)，R_1是正则表达式.$ L(R): R表示的语言 ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:1","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"正则语言封闭性 有空再写吧，本质还是构造法 ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:2","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"正则表达式与有穷自动机的等价性 我们已经知道有穷自动机识别的语言是正则语言，而后我们又认识到正则表达式(regular expression，RE)也可以来定义语言，他们两者的关系是怎样的呢？下面我们就来证明正则表达和有穷自动机是等价的。 等价，参考上一篇文章也就是说他们能够描述同样的语言。 我们要证明的也就是以下定理： Satz 3.19 (Kleene 1956) Eine Sprache L $\\in$ $\\Sigma^{*}$ ist genau dann durch einen regulären Ausdruck darstellbar, wenn sie regulär ist. 这也其实就是要我们证明：一个语言是正则的当且仅当可用正则表达式描述这个语言。 我们要做的其实也就是有一个正则表达式，我们要构造一个有穷自动机识别正则表达式产生的那些串；然后给定一个自动机，我们要构造一个正则表达式使得这个RE描述的串正好是自动机识别的串，所以我们要给定一个算法能在有穷自动机和相应的RE之间进行转换，并且这种转换是等价的。 先来看看 “$\\implies$” 方向： 这里需要做的其实是就是构造一个有穷自动机去识别相应的语言，也就是将一个正则表达式转换为自动机。关于自动机其实为了方便只需要关注NFA就可以了，因为上一篇文章中我们已经证明了DFA和NFA的等价性。 正则表达式是通过前文说的6个基本步骤来定义的，前三个是基本步骤，后三个是归纳步骤；我们只需说明对于每一个步骤如何去构造相应的有穷自动机即可完成证明。 R = a, a$\\in\\Sigma$. L(R) = {a}, N = ({$q_1$, $q_2$ }, $\\Sigma$, $\\delta$, $q_1$, {$q_2$} ), $\\delta (q_1, a)$ = ${q_2 }$, $\\delta (r, b) = \\emptyset$, 若$r\\neq q_1$或$b \\neq a$ (其实就是只有a的语言) R = $\\epsilon$. L(R) = {$\\epsilon$}, N = ( {$ q_1 $}, $\\Sigma, \\delta, q_1$, {$q_1$} ), $\\forall r, \\forall b, \\delta (r, b) = \\emptyset$. (只有一个状态$q_1$接受空串，其他串都不接受) R = $\\emptyset$. L(R) = $\\emptyset$, N = ( {$q_1 $}, $\\Sigma , \\delta , q_1, \\emptyset$), $\\forall r, \\forall b, \\delta(r,b)=\\emptyset$. (只有一个状态$q_1$，但区别不是接受状态，也即什么串也不接受) $R=(R_1 \\cup R_2)$, 这里直接上教授slides上面的图 并/alternative 这个新自动机的接受状态为原来接收状态的并，并且用一个新的初始状态来代替旧的。 $R=(R_1 R_2)$, 这里还是看图说话 连接/concatenation 这个新自动机接受状态为后一个的接受状态，而初始状态只有第一个的了。 $ R =(R_1 ^*)$, 还是直接给出图比较直观 星号/star 加入一个新的初始状态，并且本身是一个接收状态，这样可以保证接受$\\epsilon$空串，然后通过$\\epsilon$连接新旧初始状态，和接受状态。 这一个方向证毕。 再来看看 “$\\Longleftarrow$” 方向： 这里其实有两种证明方式，一种是通过定于广义非确定型有穷自动机来证明，还有一种是Tobia教授课上给出的一种算法，我们不妨都来学习学习。 先来看看Tobias教授给出的算法： Sei $M = (Q, \\Sigma ,\\delta , q_1 , F) ein DFA$. (Funktioniret analog auch für NFA) WIr konstruieren einen RE $\\gamma$ mit L(M) = L ($\\gamma $). Sei Q = {$q_1 ,…,q_n$}.Wir setzen $$ R_{ij}^{k}:= \\lbrace w \\in \\Sigma ^* | die \\ Eingabe\\ w \\ führt\\ von\\ q_i\\ in\\ q_j ,wobei\\ alle Zwischenzustände (ohne\\ ersten\\ und\\ letzten) einen\\ Index\\ \\leq k\\ haben\\ \\rbrace $$ Behauptung: Für alle $i, j \\in \\lbrace1,…,n\\rbrace und\\ k \\in \\lbrace 0,…,n \\rbrace können\\ wir\\ einen\\ RE\\ L(\\alpha {ij}^k) = R{ij}^k$ Induktion über k: k = 0: Hier gilt 再来看看另一种证法： 其实也就是定义一个广义非确定型有穷自动机（GNFA），从DFA构造等价的GFNA，从GFNA构造等价的正则表达式。 GFNA的特殊在于状态间的转移不 ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:3","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"泵引理 泵引理（Pumping Lemma für reguläre Sprachen），用来证明一个语言不是正则的神奇工具。有些语言看起来很规则，但是却不是正则语言，因为无法用有穷自动机来描述，我们用泵引理即可很有效地判断不是正则语言的情况，但却无法证明是正则语言，因为只是必要条件而不是充分条件。下面来看看泵引理具体长什么样子，既有中文版，也有德语版： 泵引理：设A是正则语言，则存在常数p（称为泵长度），使得若 s $\\in$ A且 |s| $\\geq$，则 s = xyz，并且满足下述条件： $\\forall i \\geq 0, xy^iz \\in A;$ |y| \u003e 0; |xy| $\\leq p$. 这里第一个条件其实很有意思，这其实很像前面广义非确定有穷自动消除一个顶点后出来的表达式，实际上并不是巧合，从某个状态出发读了x之后进入一个状态，这个状态自己到自己有一个箭头，并且可以通过 y 来到达，这个状态读取 z后到接受状态（这个样子其实和水泵也很像）。 接下来看看德语版： Satz 3.30 (Pumping Lemma für reguläre Sprachen) Sei R $\\subseteq \\Sigma ^*$ regulär. Dann gibt es ein n \u003e 0, so dass sich jedes z $\\in$ R mit |z| $\\geq$ n so in $z = uvw$ zerlegen lässt, dass $v \\neq \\epsilon$ $|uv| \\leq n$, und $\\forall i \\geq 0. uv^iw \\in R$. 泵引理来证明语言不是正则的其实就是要来用上面的条件来推矛盾，有时候就需要灵感来取一个特殊的s，经过泵的步骤后就不在这个语言中了，就有了矛盾。 泵引理的证明 在这个证明中我们会明白如何取这个常数 p，为什么s会被分为三段，为什么xy的长度要小于等于p。 提前剧透：这里的p其实就是一个相应自动机的状态数。 证明：设A = L(M), M = (Q, $\\Sigma ，\\delta ，q_1, F$), |Q| = p, $s = s_1s_2…s_n \\in A, n \\geq p$. 设M在s上计算 $r_1,r_2,…,r_{n+1}, \\delta({r_i, s_i}) = r_{i+1}$ $r_i$为n+1个状态，经由s的n个输入字符可到达。由于总共只有p个状态，并且不超过n，而现在有n+1个状态，每一个都是Q中的元素，由著名的抽屉原理：这n+1个状态中最多只有p种不同的状态, 也就是说至少会有两个重复的状态。 根据抽屉原理，存在 j \u003c l, 使得 $r_j = r_l , l \\leq p + 1$. 令 $x = s_1 …s_{j - 1}, y=s_j …s_{l - 1}, z=s_l …s_{n+1}$. 由于$x让M从r_1 到r_j，y让M从r_j到 r_j, z让M从r_j到r_{n+1}, 而r_{n+1}是接受状态，所以\\forall i \\geq 0, xy^iz \\in A.$ 由于$j \\neq l , 所以|y|\u003e0$. 由于$l \\leq p+1, 所以|xy| \\leq p$. 这样就完成了泵引理的证明。 Tobia教授slides上面的证明也很类似： Sei R = L(A), A = ($Q, \\Sigma , \\delta , q_0 , F$). Sei n = |Q|. Sei nun z = $a_1a_2a_3…a_m \\in R$ mit m $\\geq n$. Die beim Lesen von $z$ durchlaufene Zustandfolge sei $q_0 = p_0 \\rightarrow ^{a_1} p_1 \\rightarrow ^{a_2}p_2 … \\rightarrow ^{a_m} p_m$ Dann muss es $0 \\leq i \\leq j \\leq n geben mit p_i = p_j$. Wir teilen $z$ wie folg auf: $$ \\underbrace{a_1…a_i}u\\underbrace{a{i+1}…a_j}v \\underbrace{a{j+1}…a_m}_w $$ Damit gilt: $|uv| \\leq n$ $v \\neq \\epsilon$, und $\\forall l \\geq 0 . uv^l w \\in R$ ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:4","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"Ardens Lemma Ardens Lemma, 也叫 Arden’s rule, 用来构造这样的自动机：只有一个初始状态，且其中没有$\\epsilon$移动。先来看看数学表达式长什么样，再来翻译翻译到底说了些啥。 $$ \\text{Sind A, B und X Sprachen mit} \\epsilon \\notin \\text{A, so gilt }\\ X = AX \\cup B \\Rightarrow X = A^B \\ \\text{其中还有一个类似的推论：} Sind\\ \\alpha , \\beta \\text{ und X regulär Ausdrücke mit } \\epsilon \\notin L(\\alpha), so\\ gilt \\ X \\equiv \\alpha X | \\beta \\Rightarrow X \\equiv \\alpha ^ \\beta $$ 于是有了这些的结论： ","date":"2020-05-03","objectID":"/posts/theo/something-about-regular-language/:0:5","tags":["theory of computation","theoretical computer science"],"title":"从有穷自动机到正则语言","uri":"/posts/theo/something-about-regular-language/"},{"categories":["Notes"],"content":"探索计算机网络OSI模型中的物理层的故事.","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer_en/","tags":["computer network"],"title":"A physical layer full of opportunities","uri":"/posts/computernetwork/physical-layer_en/"},{"categories":["Notes"],"content":"Exploring the story of the physical layer in the OSI model of computer networks. Now we come to the first layer of the OSI model of computer networks: the physical layer. Here I will use the professor’s slides as a framework to briefly discuss the story of what happens in the physical layer, mainly focusing on the transformation process between digital and analog signals, which is a battle between discrete and continuous. The reason why it is full of opportunities is that there are hidden spells between discrete and continuous waiting for us to acquire. ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer_en/:0:0","tags":["computer network"],"title":"A physical layer full of opportunities","uri":"/posts/computernetwork/physical-layer_en/"},{"categories":["Notes"],"content":"Signals and Information To understand all the interesting things that happen in the physical layer (mainly mathematical stories, of course), there is still some preparatory work to be done. Let’s take a look at what the physical layer actually does. The physical layer uses the transmission medium to establish, manage and release the physical link for both ends of the communication to achieve transparent transmission of the bit stream and to ensure that the bit stream is correctly transmitted to the opposite end. The physical layer carries the bitstream in bits. I know I’ve made this clear, but you may still be walking on clouds and not know what’s going on. So let me use a real life example to help you understand better. For example, if you are talking to a friend, the content of your speech needs to be programmed by your brain, and then the content of what you are going to say is sent to your mouth, and your mouth makes the other person hear what you are saying by making a sound. And you don’t have to think about how the sound waves will be transmitted to the other person’s ears when you speak, and you can’t see how the sound waves are transmitted to the other person’s ears. Here the sound waves can be understood as a ‘bit stream’, and your mouth is providing the ‘physical layer’. The mouth is responsible for starting the conversation (establishing the link), judging the start and end of the conversation (managing the link), and ending the conversation (releasing the link). And the mouth also does not have to think about how the sound waves reach the other person’s ears, so the sound waves are transparent to the mouth. Transparency' in the network = low management cost. We know that binary codes can be conveyed through changes in the strength of a signal, and these codes are recognized as different symbols through which a variety of information can be conveyed. In information theory we can actually quantify information by introducing the concept of entropy (entropy) in physics, and the amount of information contained in the received message is also called information entropy. In fact, in 1948 Shannon did just that, by introducing the entropy of thermodynamics into informatics. We can then define information in this way. Information, on the one hand, can be understood as the uncertainty of being able to predict changes in the signal. Thus the information content of x in an alphabet X depends on the probability of occurrence of that x at the time of observation of the information carrying signal, and is then defined as $$ I(x)=-\\log_{2}{p(x)} $$ The unit is bit. The information entropy can then be given by finding the expectation： $$ H(X)=\\sum_{x\\in X} p(x)I(x) = - \\sum_{x\\in X}p(x)\\log_{2}{p(x)} $$ ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer_en/:1:0","tags":["computer network"],"title":"A physical layer full of opportunities","uri":"/posts/computernetwork/physical-layer_en/"},{"categories":["Notes"],"content":"Signaldarstellung There are two types of signals transmitted in the physical layer transmission medium, analog signals, and digital signals, examples of which are given in the following figure. Analog signal and digital signal Since the computer can only recognize digital signals, but to propagate in the WAN but in the form of analog signals (the case of fiber optics is different again, this is an afterthought), we will set up a modem to convert the digital signals into analog signals and the reverse process. And next we will discuss the details of these two processes. ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer_en/:2:0","tags":["computer network"],"title":"A physical layer full of opportunities","uri":"/posts/computernetwork/physical-layer_en/"},{"categories":["Notes"],"content":"Mathematical Basics At the beginning, it is said that the story of the physical layer is a contest between discrete and continuous. Although there is only a contest between the two relative concepts of discrete and continuous, the two can be converted into each other using the right method, which is the magical mathematical spell we are going to explain in this part. Of course if you have enough enlightenment you can comprehend the two epic level spells again. Fourier series In mathematics, the Fourier series decomposes any periodic function or periodic signal into a collection of (possibly infinite elements) simple oscillatory functions, i.e., sine and cosine functions, and is also the core of the sampling theorem we will talk about later. I give here directly the definition above the professorslides. Ein periodisches Signal s(t) lässt sich als Summe gewichteter Sinus- und Kosinus-Schwingungen darstellen. Die so entstehende Reihenentwicklung von s(t) bezeichnet man als Fourierreihe: $$ s(t)=\\frac{a_0}{2} + \\sum_{k=1}^{\\infty} {a_k\\cos{k{\\omega}t} + b_k\\sin{k{\\omega}t}} $$ Here the $a_0$ is the offset relative to the y-axis, while $a_k$ and $b_k$The two coefficients can be defined by： $$ a_k = \\frac{2}{T}\\int_0^T {s(t)\\cos{k{\\omega}t}} {\\rm d}t $$ $$ b_k = \\frac{2}{T}\\int_0^T {s(t)\\sin{k{\\omega}t}} {\\rm d}t $$ Fourier transform The Fourier transform is a linear integral transform used to transform a signal between the space-time and frequency domains. In fact, to borrow from Wikipedia, the Fourier transform is like a chemical analysis that determines the basic components of a substance; a signal comes from nature and can be analyzed to determine its basic components. Die Fourier-Transformierte einer stetigen, integrierbaren Funktion s(t) ist gegeben als $$ s(t) \\longrightarrow S(f) = \\frac{1}{\\sqrt{2\\pi}}\\int_{t=-\\infty}^{\\infty} {s(t)(\\cos{2\\pi ft}-i\\sin{2\\pi ft})} {\\rm d}t $$ Of which $i=\\sqrt{-1}$ , Of course, it can also be written in exponential form here, so I won’t go over it again。 ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer_en/:2:1","tags":["computer network"],"title":"A physical layer full of opportunities","uri":"/posts/computernetwork/physical-layer_en/"},{"categories":["Notes"],"content":"Sampling, Reconstruction and Quantification With the previous mathematical foundation, we can begin to learn about the signal processing process of sampling (Abtastung), reconstruction (Rekonstruktion), and quantization (Quantisierung), which leads to the exciting transformation between discrete and continuous. More specifically, the combination of sampling (time domain discrete) and quantization (value domain discrete) converts analog signals into digital signals, while reconstruction can be considered the inverse of sampling. The famous “Nyquist-Shannon sampling theorem”, is a basic bridge between continuous and discrete signals, and is actually more like a constraint for conversion. We will talk about this in more detail later. Abtastung Let’s start here by looking at what Wikipedia has to say： In the field of signal processing, sampling is the process of converting a signal from an analog signal in the continuous time domain to a discrete signal in the discrete time domain, implemented with a sampler. Usually sampling is performed jointly with quantization, and the analog signal is first sampled by a sampler at a certain time interval to obtain a temporally discrete signal, which is then also numerically discretized by an analog-to-digital converter (ADC) to obtain a digital signal that is both numerically and temporally discrete. The signal obtained by sampling is a discrete form of a continuous signal (for example, a real-life signal indicating pressure or speed). Continuous signals are typically sampled by an analog-to-digital converter (ADC) at regular intervals, when the value of the continuous signal at that point in time is represented as a discrete, or quantized, value. The discrete form of the signal thus obtained often introduces some errors into the data. The errors come from two main sources, the sampling frequency related to the spectrum of the continuous analog signal, and the word length used for quantization. The sampling frequency refers to the frequency at which a continuous signal is sampled. It represents the accuracy of the discrete signal in and time and spatial domains. The word length (number of bits) is used to represent the value of the discrete signal, and it reflects the accuracy of the signal’s magnitude. Let’s see what the professor’s slide says: Das Signal s(t) wird mittels des Einheitsimpulses (Dirac-Impulses) $\\sigma[t]$ in äquidistanten Abständen $T_a$ (Abtastintervall) für n $\\in$ Z abgetastet: $$ \\hat{x}=s(t) \\sum_{n=-\\infty}^{\\infty}\\sigma[t-nT_a]= \\begin{cases} 1, \u0026 t=nT_a \\newline 0, \u0026 sonst\\ \\end{cases} $$ Da $\\hat{s}(t)$ nur zu den Zeitpunkten nTa für ganzzahlige n von Null verschieden ist, vereinbaren wir die Schreibweise $\\hat{s}[n]$ für zeitdiskrete aber wertkontinuierliche Signale. Zeitkontinuierliches Signal und Abtastwerte Reconstruction In this process the digital signal is converted to an analog signal as if the sampling process were reversed, called demodulation, and on an ideal system the output is instantaneously changed to that intensity every time a new data is read after a fixed time of sampling. After such instantaneous conversion, the discrete signal will essentially have a large amount of high frequency energy and harmonics associated with the multiple of the sampling rate. To destroy these harmonics and make the signal smooth, the signal must pass through some analog filter that suppresses any energy that is outside the expected frequency domain. The multiplication in the time domain corresponds to the convolution in the frequency domain. $$ s(t) \\delta [t -nT] \\rightarrow \\frac{1}{T}S(f)*\\delta[f - n/T] $$ Reconstruction Shannon’s Theorem Shannon’s theorem gives the relationship between the upper limit of the transmission rate of channel communication and the signal-to-noise ratio and bandwidth. Abtasttheorem von Shannon und Nyquist Ein auf |f | $\\leq$ B bandbegrenztes Signal s(t) ist \u003evollständig durch äquidistante Abtastwerte ˆ s[n] beschrieben, sofern diese nic","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer_en/:2:2","tags":["computer network"],"title":"A physical layer full of opportunities","uri":"/posts/computernetwork/physical-layer_en/"},{"categories":["Notes"],"content":"Transmission channel For noiseless, M channels, we will have $M = 2^N$ kinds of distinguishable symbols, how does the achievable data rate vary? Let us first recall the entropy. Assume that the source transmits all signals with the same probability such that the entropy (and hence the average information) of the source is maximum. For a transmission rate over a channel of width B, we obtain the maximum transmission rate of Harleys Gesetz $C_H = 2B \\log_{2}(M) bit$ There is also a new definition: Signal Power (Signallesitung) The expected value of the square of the signal amplitude corresponds to the square of the signal power. The amplitude of the variance (dispersion) signal corresponds to the signal power without the DC component and represents the power of the information-carrying volume signal. …… ","date":"2020-05-02","objectID":"/posts/computernetwork/physical-layer_en/:3:0","tags":["computer network"],"title":"A physical layer full of opportunities","uri":"/posts/computernetwork/physical-layer_en/"},{"categories":["Notes"],"content":"有穷自动机笔记1","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa_en/","tags":["theory of computation","theoretical computer science"],"title":"DFA and NFA -- 1","uri":"/posts/theo/something-about-dfa-and-nfa_en/"},{"categories":["Notes"],"content":"Some understanding of the formal definition of DFA and NFA, as well as the proof of regular operation closure and the equivalence of DFA and NFA, that is, the story of DFA belongs to NFA, is the special case and regular story yet. ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa_en/:0:0","tags":["theory of computation","theoretical computer science"],"title":"DFA and NFA -- 1","uri":"/posts/theo/something-about-dfa-and-nfa_en/"},{"categories":["Notes"],"content":"Some insights into deterministic finite automata (DFA) For deterministic finite automatons, we can give the corresponding formal definition, borrowed here from the one given by professor in his class. Ein deterministischer endlicher Automat (deterministic infinite automaton, DFA) M = (Q, ∑, δ, q0, F), besteht aus einer endlichen Menge von Zuständen Q, einem (endlichen) Eingabealphabet ∑, einer (totalen!) Übergangsfunktion δ: Q × ∑ → Q, einem Startzustand q0 ∈ Q, und einer Menge F ⊆ Q von Endzuständen (akzeptierenden Zust.) This means that we first have an infinite set of states Q including all the states of the automaton, then an input alphabet ∑ including all possible inputs, a transfer function δ mapping from the present state receiving an input to a new state (there is an extended transfer function to be described later), an initial state q0 indicating where to start from, and a set F of the received states of the automaton as a subset of Q. Regarding this extended transfer function we can define it similarly: δ: Q × ∑* → Q Here ∑* represents a string consisting of ∑, which means that the automaton can also react to the input of a string, as defined for convenience. The language accepted by the device is noted as. L(M):= {w∈∑*|δ(q0, w)∈F} It means that the state in which the input string w is read from the initial state is an accepted state, and then the string is accepted. A language recognized by DFA is also called a regular language. ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa_en/:1:0","tags":["theory of computation","theoretical computer science"],"title":"DFA and NFA -- 1","uri":"/posts/theo/something-about-dfa-and-nfa_en/"},{"categories":["Notes"],"content":"Some insights into non-deterministic finite automata (NFA) For non-deterministic finite automaton the difference from deterministic lies in the non-determinism: the next state can not be uniquely determined, ε-shifts can be made, multiple choices (with 0 choices), let’s still look at the definition on Professor’s slides. Ein nichtdeterministischer endlicher Automat (nondeterministic infinite automaton, NFA) ist ein 5-Tupel N = (Q, ∑, δ, q0, F), so dass Q, ∑, q0 und F sind wie bei einem DFA δ: Q × ∑ → P(Q) P(Q) = Menge aller Teilmengen von Q = 2^Q. Alternative: Relation δ ⊆ Q × ∑ × Q. Note that the input alphabet ∑ here is the sum of the original ∑ and ε, i.e., the symbol ε of length 0 is added; then for the transfer function δ here, a symbol is read into a state in the current state, and because there are multiple choices of uncertainty, multiple states are entered, which is represented by a power set, i.e., the new state here is the state of several Qs. Similarly here for the extension of the transfer function δ one can rewrite Q as P(Q), extending the original one state to a set of several states, which constitutes a multiple backup. The language accepted by the device is noted as L(N):= {w∈∑*|δ({q0}, w) ∩ F ≠ Ø} Now it is natural to wonder what the story is between DFA and NFA, and whether DFA and NFA have the same capabilities? That is, do they recognize the same language? We know that DFA recognizes the same language that NFA recognizes, so does DFA recognize the same language that NFA recognizes? This brings us to the question of their equivalence. ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa_en/:2:0","tags":["theory of computation","theoretical computer science"],"title":"DFA and NFA -- 1","uri":"/posts/theo/something-about-dfa-and-nfa_en/"},{"categories":["Notes"],"content":"Closure of regular operations We want to go to prove the equivalence of DFA and NFA or first prove the closure of the regular operation (have time to write it again, mainly or construction method) ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa_en/:3:0","tags":["theory of computation","theoretical computer science"],"title":"DFA and NFA -- 1","uri":"/posts/theo/something-about-dfa-and-nfa_en/"},{"categories":["Notes"],"content":"Equivalence of DFA and NFA We start with a definition of “equivalence”, i.e., two machines recognize the same language, they may have the same function but different internal structure; the number of states, the transfer function may not be the same. So what we have to prove is that every NFA has equivalent DFA.(There are some digressions here. If it is a story of deterministic and non-deterministic of push-down automata or Turing machines, we need to explore it again) Here we need to use the construction method to prove. Proof idea: for a given NFA, construct the equivalent DFA and use the DFA to model the NFA, i.e., let the DFA remember all branches of the NFA (theoretically feasible because the k states of the NFA are infinite, and so is the subset of all possible states 2^k), while introducing the concept of ε-closure for each subset of states, the new subset of states attainable by an ε-move. A proof is given below. Say NFA N = (Q, ∑, δ, q0, F), construct DFA M = (Q', ∑, δ', q0', F'), L(M) = L(N). Make Q' = P(Q). for R∈Q' and a∈∑, E(R)={q|Move along 0 or more ε from R up to q}; δ'(R,a)=∪(r∈R)(R ∩ F ≠ Ø). ","date":"2020-04-25","objectID":"/posts/theo/something-about-dfa-and-nfa_en/:4:0","tags":["theory of computation","theoretical computer science"],"title":"DFA and NFA -- 1","uri":"/posts/theo/something-about-dfa-and-nfa_en/"}]